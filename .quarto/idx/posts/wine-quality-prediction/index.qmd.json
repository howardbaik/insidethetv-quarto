{"title":"Wine Quality Prediction","markdown":{"yaml":{"title":"Wine Quality Prediction","description":"On June 14th, 2018, I participated in a Data Hackathon as part of the REU program at George Mason University.","author":"Howard Baek","date":"2018-06-18","categories":["Research"],"image":"thumbnail.jpg"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nOn June 14th, 2018, I participated in a Data Hackathon as part of the REU program at George Mason University. It was my first ever hackathon and I was excited to finally participate in one. It lasted approximately 4 hours, from 10am to 2pm. Our team, consisting of three undergraduate students, worked with the famous Wine Quality dataset, which is hosted by [University of California Irvine's Center for Machine Learning and Intelligent Systems](https://archive.ics.uci.edu/ml/index.php). The goal of the hackathon was to accurately predict the Quality variable (\"Good\"= 1 or \"Bad\" = 0)\n\n## Dataset\n\nI first import the dataset and observe it.\n\n```{r, warning=FALSE, message=FALSE}\n# Load tidyverse and caret package\nlibrary(tidyverse)\nlibrary(caret)\n\n# Import training / test data\nwine_train <- read_csv(\"wine_train.csv\")\nwine_test <- read_csv(\"wine_test.csv\")\n```\n\n```{r}\nglimpse(wine_train)\nglimpse(wine_test)\n```\n\nTraining data has 799 observations and 12 variables, including the target variable, Quality, while the testing data has 800 observations and exactly the same attributes except Quality.\n\n## Data Manipulation\n\n```{r}\n# Change columns names- Take out single quotations and underscores from names \nnames(wine_train) <- gsub(\"'\", '', names(wine_train))\nnames(wine_train) <- gsub(\" \", \"_\", names(wine_train))\nnames(wine_train)\n\nnames(wine_test) <- gsub(\"'\", '', names(wine_test))\nnames(wine_test) <- gsub(\" \", \"_\", names(wine_test))\nnames(wine_test)\n```\n\n```{r}\n# Change values in Quality column: \"B\" = 0 & \"G\" = 1\nwine_train <- wine_train %>% \n  mutate(Quality = ifelse(Quality == \"B\", 0, 1))\n\n# Observe number of 0s and 1s\ntable(wine_train$Quality)\n```\n\n<br>\n\n## Feature Selection\n\nI first wanted to select the relevant and useful features by means of feature selection in the [caret package](https://topepo.github.io/caret/index.html), a popular R package for statistical machine learning. This tutorial got me started: https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/\n\n```{r}\n# Feature Selection #1\nset.seed(7) # Bring me luck\ntrain_cor <- cor(wine_train[, -length(names(wine_train))])\n\n# summarize the correlation matrix\nprint(train_cor)\n\n# find attributes that are highly corrected (ideally >0.75)\nhigh_cor <- findCorrelation(train_cor, cutoff=0.5)\n\n# print indexes of highly correlated attributes\nprint(high_cor)\n```\n\n-   Index 1 = `fixed_acidity`\n-   Index 2 = `citric_acid`\n-   Index 3 = `total_sulfur_dioxide`\n\n## Model Fitting\n\nSince `fixed_acidity`, `citric_acid` and `total_sulfur_dioxide` are highly correlated (redundant), I only used one of these features (`total_sulfur_dioxide`) and disposed of the two redundant ones (`fixed_acidity`, `citric_acid`). At this point, I formulated a hypothesis: a model without redundant features performs better than a model with redundant features. Let's find out if this is true.\n\n*Since the target variable is binary, I fit a logistic regression.*\n\n```{r}\n# Logistic Regression\nwine_train$Quality <- factor(wine_train$Quality, levels = c(0, 1))\ntrain_log <- createDataPartition(wine_train$Quality, p=0.6, list=FALSE)\ntraining <- wine_train[train_log, ]\ntesting <- wine_train[ -train_log, ]\n\n# Hypothesis: Try logistic regression with all the predictor variables\nmod_log <- train(Quality ~ .,  data=training, method=\"glm\", family=\"binomial\")\n\nexp(coef(mod_log$finalModel))\npred <- predict(mod_log, newdata=testing)\naccuracy <- table(pred, testing$Quality)\nsum(diag(accuracy))/sum(accuracy)\n\n# Hypothesis: Try logistic regression without the redundant variables\n# Try logistic regression without highly correlated variables\nmod_log_2 <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol,  data=training, method=\"glm\", family=\"binomial\")\n\npred_2 <- predict(mod_log_2, newdata = testing)\naccuracy_2 <- table(pred_2, testing$Quality)\nsum(diag(accuracy_2))/sum(accuracy_2)\n```\n\nThe first hypothesis yields an accuracy rate of 71.5%! while the first hypothesis yields 70.8%! Apparently, including all the variables yields higher accuracy.\n\n## Cross Validation\n\nAt this point, I looked on the tutorial page for the caret package to learn how to cross validate. I learned about `trainControl`, a function \"used to specify the type of resampling\". The parameter, `method`, specifies `repeatedcv`, which stands for repeated cross validation.\n\n```{r}\n# Cross validation on the second model where I took out the redundant variables\nctrl <- trainControl(method = \"repeatedcv\", repeats = 10)\n\n# Train logistic regression model\nmod_log_2_ctrl <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                     total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                     trControl = ctrl, method=\"glm\", family=\"binomial\")\npred_2_ctrl <- predict(mod_log_2, newdata = testing)\naccuracy_2_ctrl <- table(pred_2, testing$Quality)\nsum(diag(accuracy_2_ctrl))/sum(accuracy_2_ctrl)\n\n```\n\nI got the same accuracy, which means that I didn't use cross validation properly...I'll have to learn more.\n\n## Advanced Models\n\nIn an effort to achieve a higher accuracy score, I looked for more accurate and powerful models, such as XgBoost, Random Forests, etc.\n\n```{r, eval = F}\n# XgBoost -----------------------------------------------------------------\nmod_xgboost <- train(Quality ~ ., data=training, \n      trControl = ctrl, method=\"xgbTree\", family=\"binomial\")\npred_xgboost <- predict(mod_xgboost, newdata = testing)\nacc_xgboost <- table(pred_xgboost, testing$Quality)\nsum(diag(acc_xgboost))/sum(acc_xgboost)\n# 70.8%\n\n# Random Forest-----------------------------------------------------------------\nmod_rf <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                  total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                trControl = ctrl, method=\"rf\", family=\"binomial\")\npred_rf <- predict(mod_rf, newdata = testing)\nacc_rf <- table(pred_rf, testing$Quality)\nsum(diag(acc_rf)) / sum(acc_rf)\n# 80.6% is an improvement!\n\n# Logit Boost-----------------------------------------------------------------\nmod_logit <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                  total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                trControl = ctrl, method=\"LogitBoost\")\npred_logit <- predict(mod_logit, newdata = testing)\nacc_logit <- table(pred_logit, testing$Quality)\nsum(diag(acc_logit)) / sum(acc_logit)\n# 72.1%\n\n# svmRadial-----------------------------------------------------------------\nmod_svm <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                     total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                   trControl = ctrl, method=\"svmRadial\")\npred_svm <- predict(mod_svm, newdata = testing)\nacc_svm <- table(pred_svm, testing$Quality)\nsum(diag(acc_svm)) / sum(acc_svm)\n# 75.2%\n\n# LMT-----------------------------------------------------------------\nmod_svm_linear <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                   total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                 trControl = ctrl, method=\"svmLinearWeights2\")\npred_svm_linear <- predict(mod_svm_linear, newdata = testing)\nacc_svm_linear <- table(pred_svm_linear, testing$Quality)\nsum(diag(acc_svm_linear)) / sum(acc_svm_linear)\n```\n\n## Conclusion\n\nI learned about a totally new field in machine learning. Importantly, is very interesting and motivating since coming up with machine learning models feels like creating and refining a crystal ball that shows the future. In the future, I plan on reading through [this comprehensive tutorial of the caret package](https://topepo.github.io/caret/index.html), take machine learning courses on [DataCamp](www.datacamp.com), and hope to learn from the mistakes I made during this hackathon.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.245","editor":"visual","theme":"litera","title-block-banner":true,"comments":{"utterances":{"repo":"howardbaek/blog-comments"}},"title":"Wine Quality Prediction","description":"On June 14th, 2018, I participated in a Data Hackathon as part of the REU program at George Mason University.","author":"Howard Baek","date":"2018-06-18","categories":["Research"],"image":"thumbnail.jpg"},"extensions":{"book":{"multiFile":true}}}}}
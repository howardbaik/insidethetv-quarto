[
  {
    "objectID": "posts/building-nhl-play-by-play-app/index.html",
    "href": "posts/building-nhl-play-by-play-app/index.html",
    "title": "Building the NHL PBP App in Shiny",
    "section": "",
    "text": "I created a web application for interactively visualizing shot data for all games in the 2017-2018 season. In this article, I will walk through a month-long process building the National Hockey League Play-by-Play App from scratch, giving a behind-the-scenes look."
  },
  {
    "objectID": "posts/building-nhl-play-by-play-app/index.html#the-process",
    "href": "posts/building-nhl-play-by-play-app/index.html#the-process",
    "title": "Building the NHL PBP App in Shiny",
    "section": "The Process",
    "text": "The Process\nWhat started this project was this #rstats Shiny contest tweet. Shiny is a R package built by RStudio for creating interactive web applications. It allows R programmers to create web applications without having to exclusively code in HTML, CSS or JavaScript. I had looked at several sports visualizations (e.g. Ryo’s Visualize the World Cup) and wanted to create something similar in hockey. This announcement provided the motivation for me to start.\nI started sharpening my Shiny skills by taking DataCamp’s Shiny Course. I particularly found Chapter 2 (Inputs, outputs, and rendering functions) and Chapter 3 (Reactive Programming) helpful in reminding myself of the essence of Shiny. They are great visual learning resources and I highly recommend beginners in Shiny take this course.\nNow, I focused on the structure of my application. The organization of a product is instrumental in its usability, so I wanted to get it right. I looked at the Shiny Application Layout Guide and decided to go with the Grid Layout, which contains a plot at the top and parameters of the plot at the bottom in a three column format. This is the best organization for focusing the users on the animation at the top. The secondary features, which are the parameters controlling the plot, are stationed at the bottom.\nNow, to the animation. I relied on Ryo’s World Cup animations, which was rendered in gganimate, a R package for animations compatible with ggplot2. Unfortunately, unlike Ryo’s dataset, my dataset doesn’t contain coordinate data points with the location of each player over time. Rather, my Play-by-Play, Real Time Scoring System dataset contains shot location:\n\n\n\nFigure 1: Snapshot of raw shot data by Corsica\n\n\nIf the NHL had tracked real time coordinate data like the NFL, I could have created a fluid animation like this:\n\n\n\n\nFigure 2: Tyreek Hill’s TD reception during Week 1 of 17/18 season\n\n\n\nSo, here is a hack I came up with. First, I’ve normalized the shot locations so that all shots taken by the home team were shown on the right and shots taken by the away team were shown on the left. Then, after every shot location data, I input (x,y) coordinates (82, 0) and (-82, 0) to mark the location of both nets. Next, I created a column called event_index that groups each pair of shot data (1 row for shot location, 1 row for net location). I then created a column called event_frame that numerates all the rows. Last, I used group aesthetic on event_index and added transition_components(time = event_frame) to render the animation.\n\n\n\nFigure 3: Data Processed for animation\n\n\nThis was all great, but I realized that the gganimate package doesn’t yet work with Shiny. There is no function designed to render gganimate animations on Shiny. In other words, there was no natural way to put my animations on my end product, which was a huge concern.\nThis StackOverflow answer was super helpful in coming up with another hack. It recommended saving the animation as a .gif file and returning the file as a list along with the dimensions of the animation. There is one drawback to this method though: the animation looks stretched out if I increase the width too much, and it moves downward if I increase the height too much. As a result, what I currently have is the best I could come up with: high image resolution and optimal placement.\nThe animation happens on a NHL ice rink created by War On Ice. I added “reactive” team logos on Shiny to clearly indicate which side is the home/away side. Also, in the app, users need to input the official game ID in order to navigate between games. In order to facilitate this process, I included a datatable of all the game IDs, game dates, home teams, and away teams next to the animation. That way, the user can find the desired game by searching through game dates or teams, locate the right Game ID, and render the right animation.\n\n\n\n\nFigure 4: Animation of a Regular Season Game between the Toronto Maple Leafs and the Winnipeg Jets\n\n\n\nNow, the other visualizations. I took a long, hard look at the dataset and thought about which columns to make use of. I thought the shot distance was pretty interesting, so I created a histogram of the shot distance. This illustrates the number of shots a team took at a certain distance from the net. To help the user interpret the distances, I labelled the location of the faceoff circles, blue line, and the red line. Furthermore, expected goal probability is a frequently occuring metric in hockey analytic discussions. I thought it would be interesting to see its change throughout the game. As a result, I animated expected goal probabilities for each team. This plot generated the most buzz.\n\n\n\n\nFigure 5: Animation of Expected Goal Probability during a Regular Season Game between the Toronto Maple Leafs and the Winnipeg Jets\n\n\n\nLast, I wanted to include a summary of the game by showing the boxscore. However, I ran into too many roadblocks with html / css, so I decided to simply show the nhl.com official recap.\nSome neat features I added to the app include a short tour using the rintrojs package. When the user presses the Help bottom on the top right corner, Shiny gives a short tour, explaining what each of the parameters do. Also, the “Share” button allows users to easily share the app with a custom message I included and the “Code” button redirects users to the Github repo.\n\n\n\nFigure 6: Illustration of the rintrojs package"
  },
  {
    "objectID": "posts/building-nhl-play-by-play-app/index.html#result",
    "href": "posts/building-nhl-play-by-play-app/index.html#result",
    "title": "Building the NHL PBP App in Shiny",
    "section": "Result",
    "text": "Result\nThe final product is available here: NHL Play-by-Play App\nUpdate: This app received a Honorable Mention from RStudio’s 1st Shiny Contest"
  },
  {
    "objectID": "posts/my-r-journal/index.html",
    "href": "posts/my-r-journal/index.html",
    "title": "My R Journal",
    "section": "",
    "text": "Tips and tricks I learn to program in R."
  },
  {
    "objectID": "posts/my-r-journal/index.html#r-package",
    "href": "posts/my-r-journal/index.html#r-package",
    "title": "My R Journal",
    "section": "R package",
    "text": "R package\n\nDemystifying Dependencies\nChoose whether package dependency is an “Imports” or a “Suggests” package: https://r-pkgs.org/dependencies-mindset-background.html#sec-dependencies-imports-vs-suggests\n“Imports”: Read Section 11.1 ~ 11.4 from the R Packages book: https://r-pkgs.org/dependencies-in-practice.html#confusion-about-imports.\n“Suggests”: Read Section 11.5 from the R Packages book: https://r-pkgs.org/dependencies-in-practice.html#sec-dependencies-in-suggests\n\n\nHandy functions\n\nWrite roxygen tag into the source code of package: usethis::use_import_from(\"pkg\", \"function\")\nAdd package to Imports field of DESCRIPTION: usethis::use_package(\"pkg\")\n\n\n\nSubmitting to CRAN\n\nFirst release:\n\nusethis::use_news_md() creates a NEWS.md file with “News” items\nusethis::use_cran_comments() initiates a file to hold submission comments for your package\nUpdate instructions on README to install package from Github to from CRAN, in anticipation of your package’s acceptance.\nProofread Title: and Desription: fields of DESCRIPTION as they are real hotspots for nitpicking during CRAN’s human review. Read advice given in Section 9.2 of the R Packages book.\nCheck that all exported functions have @returns and @examples\nCheck licenses\nReview https://github.com/DavisVaughan/extrachecks (extra ad-hoc checks that CRAN does that are not checked for by devtools::check())\n\nTo keep tabs on what other package maintainers are talking about, https://stat.ethz.ch/mailman/listinfo/r-package-devel\n\n\nUpdating a package already on CRAN:\n\nCheck current CRAN check results\nIf you have depcreated functions, check Gradual deprecation\nUpdate NEWS with changes to the package.\nRun urlchecker::url_check() and devtools::build_readme()\n\n\n\nAlways Run No Matter What:\n\ndevtools::check(remote = TRUE, manual = TRUE)\ndevtools::check_win_devel()\nrhub::check_for_cran() (There are some bug-related messages, like Skipping checking HTML validation: no command 'tidy' found, Found the following files/directories: NULL or lastMiKTeXException)\n\n\n\nReverse Dependency Check:\nRunning for the first time: usethis::use_revdep() Then, run revdepcheck::revdep_check(num_workers = 4)\n\n\nReady to Submit:\n\nBump the version number in DESCRIPTION\ndevtools::submit_cran(). After a successful upload, you should receive an email from CRAN within a few minutes. This email notifies you, as maintainer, of the submission and provides a confirmation link.\nAt the confirmation link, you are required to re-confirm that you’ve followed CRAN’s policies and that you want to submit the package. If you fail to complete this step, your package is not actually submitted to CRAN!"
  },
  {
    "objectID": "posts/my-r-journal/index.html#apis",
    "href": "posts/my-r-journal/index.html#apis",
    "title": "My R Journal",
    "section": "APIs",
    "text": "APIs\n\nGoogle APIs and OAuth 2.0\nSource: https://developers.google.com/identity/protocols/oauth2\nGoogle APIs use the OAuth 2.0 protocol for authentication and authorization. To begin, obtain OAuth 2.0 client credentials from the Google Cloud. See this page on getting your own API credentials. Then your client application requests an access token from the Google Authorization Server, extracts a token from the response, and sends the token to the Google API that you want to access.\nFor an interactive demonstration of using OAuth 2.0 with Google (including the option to use your own client credentials), experiment with the OAuth 2.0 Playground.\nBefore your application can access private data using a Google API, it must obtain an access token that grants access to that API. A single access token can grant varying degrees of access to multiple APIs. A variable parameter called scope controls the set of resources and operations that an access token permits.\nSome requests require an authentication step where the user logs in with their Google account. After logging in, the user is asked whether they are willing to grant one or more permissions that your application is requesting. This process is called user consent.\nSee this diagram to get an overview of the entire process of calling a Google API.\nFurther Reading on dealing with OAuth 2.0 in R:\n\nhttps://blog.r-hub.io/2021/01/25/oauth-2.0/\nhttps://googledrive.tidyverse.org/articles/bring-your-own-app.html\nhttps://www.youtube.com/watch?v=hHRFjbGTEOk\nhttps://www.youtube.com/watch?v=3pZ3Nh8tgTE"
  },
  {
    "objectID": "posts/my-r-journal/index.html#knitr",
    "href": "posts/my-r-journal/index.html#knitr",
    "title": "My R Journal",
    "section": "{knitr}",
    "text": "{knitr}\n\nChunk options\nWhen you knit a R Markdown file with the following code chunks,\nSetup code chunk (not shown because include set to FALSE):\n\nknitr::opts_chunk$set(echo = TRUE, comment = \">\")\n\n\nknitr::opts_chunk$get()[c(\"echo\", \"comment\")]\n\n> $echo\n> [1] TRUE\n> \n> $comment\n> [1] \">\"\n\n\nknitr::opts_chunk$get() shows you the default (global) options.\n\nknitr::opts_current$get()[c(\"echo\", \"comment\")]\n\n### $echo\n### [1] TRUE\n### \n### $comment\n### [1] \"###\"\n\n\nknitr::opts_current$get() shows you the options set in the chunk itself.\nThe doc gives more details:\n\nNormally we set up the global options once in the first code chunk in a document using opts_chunk$set(), so that all latter chunks will use these options. Note the global options set in one chunk will not affect the options in this chunk itself, and that is why we often need to set global options in a separate chunk."
  },
  {
    "objectID": "posts/difference-regular-season-playoffs-extended/index.html",
    "href": "posts/difference-regular-season-playoffs-extended/index.html",
    "title": "Quantifying Differences between the Regular Season and Playoffs Extended",
    "section": "",
    "text": "After several months of learning the concept of survival analysis and applying it to hockey, I published my article, Quantifying Differences between the Regular Season and Playoffs using Survival Analysis. This post is a brief analysis to answer the question: “Can I repeat my previous analysis for regular season by period?”. First, I only look at regular season data and change the treatment variable from whether the game is played during the regular season or playoffs to whether it was played in Period X vs Period Y. Then, I approach the question in a different way by keeping the treatment variable as regular season vs playoffs, but filter for the 1st, 2nd, and 3rd periods. This further shows the discrepancy in change in rates of events by period.\nFinally, I respond to some additional feedback from Ron Schultz: “Have you looked at the same comparisons, but with just the regular season games between the teams who ended up in the playoffs?”. I checked the differences between the regular season and playoffs by running my survival analysis on just the playoff teams in the 2017-2018 season. I discovered that this type of analysis yields a statistically significant difference in all event types."
  },
  {
    "objectID": "posts/difference-regular-season-playoffs-extended/index.html#hazard-ratios-period-1-vs-period-2",
    "href": "posts/difference-regular-season-playoffs-extended/index.html#hazard-ratios-period-1-vs-period-2",
    "title": "Quantifying Differences between the Regular Season and Playoffs Extended",
    "section": "Hazard Ratios: Period 1 vs Period 2",
    "text": "Hazard Ratios: Period 1 vs Period 2\n\nFirst, let’s compare event rates in the first period to the second. There are big differences in many of them: every event is statistically significant except for goals. Furthermore, they all move in the same direction: they decrease from first period to second period. The rate of hits decrease the most by almost 20%, leading me to believe that fatigue plays a factor as players slow down after 20 minutes. Long changes in the second period may contribute to fatigue as teams must skate farther to make changes.\nInterestingly, this FiveThirtyEight article, To Make The Playoffs, Hockey Teams Play Not To Win by Noah Davis and Michael Lopez, makes a different observation with this plot:\n\n\n\nThis graph illustrates the expected goal rate by minute from 2007 to 2015 in tied games and one-goal games. According to Sam Ventura’s presentation slides at Ottawa Hockey Analytics conference, “a teams’ or players’ expected goals for is equal to the sum of the goal-probabilities of all their on-ice shot-attempts”. These goal-probabilities are explained using location, distance from goal, shot type, shot feature, and transformations / interactions of these variables.\nThe Expected Goal rate shows an increasing trend from the first period to the second period in both tied games and one-goal games. On the other hand, the hazard ratio for goals indicates a decrease in the rate of goals in the second period. This means that we “expect” more goals in the second period based on Ventura’s model, but survival analysis tells us that the rate of goals decreases."
  },
  {
    "objectID": "posts/difference-regular-season-playoffs-extended/index.html#hazard-ratio-period-2-vs-period-3",
    "href": "posts/difference-regular-season-playoffs-extended/index.html#hazard-ratio-period-2-vs-period-3",
    "title": "Quantifying Differences between the Regular Season and Playoffs Extended",
    "section": "Hazard Ratio: Period 2 vs Period 3",
    "text": "Hazard Ratio: Period 2 vs Period 3\n\nNow let’s compare the second period to the third. Again we see meaningful changes across the board. Notably, the rate of hits decrease by more than 10% and the rates of goals and shots all show a statistically significant decrease. One explanation is the change of motivation in the latter part of the game. Realistically, teams consider the longevity of the regular season. It is 82 games of grueling hockey from October to April, and then possibly another two more months of playoff hockey. As a result, teams may change the intensity of play in the third period to preserve their energy level for the next game, week, month, and hopefully, the playoffs.\nThis view is supported in the FiveThirtyEight article. Francois Beauchemin reveals that his team, the Ducks, won’t take as many risks late in the third period in order to go to overtime and earn at least one point. Todd Reirden, then an assistant coach with the Capitals, agreed: “Sit back, play defense, make sure we get a point…” His late in-game tactics are to shorten the bench and play guys “he knows will stay home and limit the opposition’s attacking chances, even if that means sacrificing a shot at getting a goal”. This stay-at-home mentality in both players and coaches contribute to such statistical results."
  },
  {
    "objectID": "posts/difference-regular-season-playoffs-extended/index.html#hazard-ratio-by-period",
    "href": "posts/difference-regular-season-playoffs-extended/index.html#hazard-ratio-by-period",
    "title": "Quantifying Differences between the Regular Season and Playoffs Extended",
    "section": "Hazard Ratio by Period",
    "text": "Hazard Ratio by Period\nMy original piece found differences between play in the regular season and the playoffs. Do these results change if we split them up by period?\nIn the first period, the hazard ratio for goals is 1.007, meaning as we move from regular season to the playoffs, goals increase by approximately 0.7%. In the second period, the rate of goals decreases by 2.4%, and in the third period, decreases by 2%. These results are not statistically significant. Significantly, the rate of goals increases in the first period from the regular season to the playoffs, but decreases by roughly the same amount in the rest of the game.\nThe rate of hits increase in the playoffs independent of period: approximately 45% in the first period, 30% in the second period, and 25% in the third period. Notably, hits swing the most from the regular season to the playoffs as its corresponding point is farthest from the vertical line representing a hazard ratio of 1.0.\nWith hazard ratios of 1.087 (first period), 1.062 (second period), and 1.081 (third period), the rate of stoppages in play increase in the playoffs in all three periods. I suspect these rates help explain the consistent increase in the rate of faceoffs throughout the game; once the play is blown down, the referee needs to resume play with a faceoff. More faceoffs mean more opportunities for faceoffs set plays in the playoffs."
  },
  {
    "objectID": "posts/difference-regular-season-playoffs-extended/index.html#hazard-ratio-1718-playoff-teams",
    "href": "posts/difference-regular-season-playoffs-extended/index.html#hazard-ratio-1718-playoff-teams",
    "title": "Quantifying Differences between the Regular Season and Playoffs Extended",
    "section": "Hazard Ratio: 17/18 Playoff Teams",
    "text": "Hazard Ratio: 17/18 Playoff Teams\nIn my previous piece, I looked at data for all teams, regardless of whether they made the playoffs or not. As a result, non-playoff teams were not accounted for in my analysis looking at the difference between the regular season and playoffs. One astute reader pointed that out and suggested I run the analysis on the teams who ended up in the playoffs. This ensures that I have evenly distributed data from the regular season and the playoffs.\n\nThe average percentage of change (hazard ratio - 1.00) on 5-on-5 situations for all events is 45% while on the power play is 37%, revealing bigger discrepancy in 5-on-5 situations from the regular season to the playoffs.\nThe reason, I’d argue, is that the power play is a more structured game within the game where teams typically play in one end of the rink. Attacking teams have a set formation once they gain control of the puck, and the penalized teams have their own style. This rigidity lends itself to less change from the regular season to the playoffs since a power play situation in the playoffs can be similar to that in the regular season. On the other hand, 5-on-5 play changes more dramatically from the regular season to the playoffs as the play is more fluid, with teams exchanging chances at both ends of the rink."
  },
  {
    "objectID": "posts/difference-regular-season-playoffs-extended/index.html#key-takeaways",
    "href": "posts/difference-regular-season-playoffs-extended/index.html#key-takeaways",
    "title": "Quantifying Differences between the Regular Season and Playoffs Extended",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFrom Period 1 to Period 2, the rate of all events go down: Long changes in Period 2 may contribute to fatigue as teams have to skate farther to their benches for changes\nWe “expect” more goals in Period 2, according to Ventura’s model with location, distance from goal, shot type, and shot feature, but our survival analysis indicates this rate decreases.\nTaking into account the longevity of the regular season, teams dial down their intensity of play in the third period to preserve their energy level.\nRate of hits differ the most from regular season to playoffs.\nOn average, there is a larger discrepancy from regular season to playoffs in 5-on-5 situations since 5-on-5 play is more fluid.\n\nCode is available here\nThe author would like to thank Sam Ventura for his contribution to this article."
  },
  {
    "objectID": "posts/summary-data-mining-paper/index.html",
    "href": "posts/summary-data-mining-paper/index.html",
    "title": "Summary of an Educational Data Mining Paper",
    "section": "",
    "text": "This is a summary of the paper, Discovery and Temporal Analysis of Latent Study Patterns in MOOC Interaction Sequences, written by Mina Shirvani Boroujeni, who at the time was a PhD Candidate at EPFL but has now become a Data Scientist at Expedia Group in Switzerland, and Pierre Dillenbourg, Professor of Learning Technologies at EPFL.\nPreviously, the authors have looked into learners’ online participation patterns across time, studied patterns in timing of study sessions and methods to quantify regularity and social dimensions (researched evolution of social interactions and changes in learners’ roles in MOOC discussion forums over time)."
  },
  {
    "objectID": "posts/summary-data-mining-paper/index.html#purpose-of-paper",
    "href": "posts/summary-data-mining-paper/index.html#purpose-of-paper",
    "title": "Summary of an Educational Data Mining Paper",
    "section": "Purpose of paper",
    "text": "Purpose of paper\nAccording to the researchers, they “investigate MOOC study patterns and perform temporal analysis of learners’ longitudinal behaviors… we aim to identify learners’ study patterns during assessment periods, that is their learning sequences from the opening time of an assignment until the submission deadline”.\n\nResearch question: What are the different study patterns exhibited by learners during MOOCs assessment periods and how do learners’ study patterns evolve over time?*"
  },
  {
    "objectID": "posts/summary-data-mining-paper/index.html#dataset",
    "href": "posts/summary-data-mining-paper/index.html#dataset",
    "title": "Summary of an Educational Data Mining Paper",
    "section": "Dataset",
    "text": "Dataset\nThe researchers used data of the interaction logs of students in Functional Programming Principles in Scala which is an undergraduate engineering MOOC (Massive Open Online Course) produced by EPFL University, a university in Switzerland. The course contains seven video lectures and six assignments where “assessment periods (assignment release to hard deadline) varied between 11 to 18 days”. Also, “the dataset includes three categories of events, describing learners??? interaction with videolectures (play, pause, download, seek, change speed), assignments(submit) and discussion forums (read, write, or vote a message)”\n\nData Preprocessing\nThe researchers split the full time period of interaction logs into “subsequences corresponding to each assessment period”. They filtered out learners who were inactive (active in less than three assessment periods). AS a result, the study contains “interaction subsequences of 7527 learners”."
  },
  {
    "objectID": "posts/summary-data-mining-paper/index.html#hypothesis-driven-method",
    "href": "posts/summary-data-mining-paper/index.html#hypothesis-driven-method",
    "title": "Summary of an Educational Data Mining Paper",
    "section": "Hypothesis-Driven Method",
    "text": "Hypothesis-Driven Method\nThis method involves the researchers labelling each students’ activity sequences and then performing clustering.\nIn order to identify patterns, the researchers categorized the sub-sequences according to two criteria:\n\nWhether the learner starts off by watching a video or submitting an assignment\nWhether the learner submits the assignment before the deadline.\n\nThese two criteria yielded the following study patterns:\n\nV_start: Learner watched the video(s) before submitting the assignment\nA_start: Learner submitted the assignment without having watched the corresponding video(s)\nAudit: Learner watched the video(s) but did not submit the assignment\nInactive: Learner did not watch the video(s) and did not submit the assignment.\n\nThen, the researchers went on to apply hierarchial agglomerative clustering to extract different patterns of learner profiles. .\n\nResults:\nStudy pattern distribution:\n\n69% = learners watch videos before submitting an assignment.\n10% = learners skip video lectures and directly submit assignments.\n8% to 18% = Auditing students increase towards the end of the course\n4% to 20% = Proportion of Inactive students increases.\nLearners who start by watching videos start their learning sequence earlier\nInterestingly, learners in A_start sessions are less likely to attempt an assignment multiple times and only 6% of A_start learners access lectures after submitting the assignment. Thus, this signifies that A_start learners are likely to know the assignment topic beforehand.\n\n\nFixed Study Pattern:\n\nComprises 53% of learners who have identical study patterns.\nCluster 1: Comprises 44% of participants who rely on lectures for learning.\nCluster 2: Comprises 2% of participants who do not watch the videos before submitting any assignments.\nCluster 3: Comprises of auditing students who do not submit any of the assignments, but follow most of the videos.\n\n\n\nChanging Study Pattern:\n\nComprises 47% of learners who change their approach at least once during the course.\nCluster 4: Mainly Assignment_Start Approach, but in first and last assignments, they watch videos before submitting\nCluster 5: Main approach is V_start but skip videos in one or two assignments during course. Interesting point: Start time of learning sequences is closer to assignment deadline, which means proximity of deadline makes them change study approach\nCluster 6: Mainly watch videos first but in last two periods, they submit assignments without watching videos. Achieve nearly complete grades in the first four assignments so these learners are likely to receive a high final grade even without receiving the highest score in the reamining assignments. However, more information about learners’ experience and conditions is required to precisely determine the factors triggering changes in learners’ study approaches\nCluster 7 and 8: Watch the videos in the first few assignments, but then loses motivation. Cluster 7: submits nearly half of the assignments, but Cluster 8: submit only the first one or two, before switching to the auditing state.\nClusters 9, 10, 11: Start with V_start approach, change to Audit state, and finally wstop watching videos and drop out. Cluster 9: Submit first four assignments and drop out, but in Cluster 10 and 11: stop doing assignments and eventually drop out."
  },
  {
    "objectID": "posts/summary-data-mining-paper/index.html#data-driven-method",
    "href": "posts/summary-data-mining-paper/index.html#data-driven-method",
    "title": "Summary of an Educational Data Mining Paper",
    "section": "Data-Driven Method",
    "text": "Data-Driven Method\nThis method invovles unsupervised learning to “discover and track latent study patterns from students’ interaction sequences”.\nIt consists of four steps: 1) Activity Sequence Modeling 2) Distance computation 3) Clustering (Hierarchial Agglomerative Clustering) 4) Cluster Matching\n\nResult\nIdentification of 13 different study patterns in Table3.\nFigure 3 = Transition Probabilities between different study patterns. Patterns 10 and 7 are the most predictable study patterns. Pattern 11, inactive learners, is repeatable with a probability of 0.6 Pattern 10 represents a similiar approach to A_start Pattern 7 is the most popular (strongest connections)."
  },
  {
    "objectID": "posts/summary-data-mining-paper/index.html#conclusion",
    "href": "posts/summary-data-mining-paper/index.html#conclusion",
    "title": "Summary of an Educational Data Mining Paper",
    "section": "Conclusion",
    "text": "Conclusion\nIn this paper, Boroujeni and Dillenbourg explore two methods to answer their research question: hypothesis method and data-driven method. The hypothesis method reveals that learning approaches definitely change in different manners. Some change to another method and then revert back to their initial method whereas others permanently switch to the new approach. The data-driven method is an unsupervised learning method that uses “action sequences as input”. This revealed 13 different study patterns. Importantly, this research can be “used for analysis of learners’ activities during the course duration”, opening doors to real-time intervention of students facing difficulties. While this paper seeks to analyze different study patterns and their evolutions, our paper will investigate specific patterns that lead to student dropouts. A similar approach is briefly covered in the paper where Table 1 shows the percentage of students who passed the course. This paper only looks into different study patterns and their evolutions, but does not address student retention with respect to the different study patterns students have. On the other hand, we will focus on the problem of student dropouts from MOOCs and its factors using data mining algorithms such as clustering and classification.\nOur work will directly build on this paper; as mentioned in the Discussion Section, we will capture an overview of the captured behaviors and study pattern sequences into an analytic dashboard to enable intervention and improvement of the course materials by instructors. We will create it with the Shiny package in R. Shiny allows for interactive visualization of data, which will definitely aid instructors in understanding their students and helping those in need."
  },
  {
    "objectID": "posts/analyzing-twitter-accounts/index.html",
    "href": "posts/analyzing-twitter-accounts/index.html",
    "title": "Analyzing Twitter Accounts",
    "section": "",
    "text": "Motivated by #tidytuesday, I will be working with the entire trove of #rstats tweets from its inception on September 7th, 2008 to the most recent one on December 12th, 2018.\nThis tweet inspired me to use K-Means Clustering, an unsupervised learning method I know a little about, but would like to learn more. I grouped twitter accounts by popularity measure, a quick and easy way (I came up with it) to quantify the popularity of a Twitter account:\n\nPopularity Measure = Likes + Rewteets + Number of Followers"
  },
  {
    "objectID": "posts/analyzing-twitter-accounts/index.html#analysis",
    "href": "posts/analyzing-twitter-accounts/index.html#analysis",
    "title": "Analyzing Twitter Accounts",
    "section": "Analysis",
    "text": "Analysis\nTo start off, I filtered only for #rstats tweets in 2018, tweets by accounts with more than 400 followers, and selected the following columns: screen_name, favorite_count(Likes), retweet_count, followers_count. Hence, I have rstats_reduced.csv\nFirst, I load packages and import my dataset:\n\nlibrary(tidyverse)\nlibrary(cluster) # For Silhouette Method\nlibrary(broom)\nlibrary(plotly) # For Interactive Visualization\ntheme_set(theme_light()) # Set default theme\n\nrstats <- read_csv(\"rstats_reduced.csv\")\n\nFor the rest of this article, I follow a clear tutorial in the UC Business Analytics R Programming Guide by Bradley Boehmke.\n\nMake rows as observations and columns as variables\n\n\n# Find the mean of Likes and Retweets\nrstats_clustering <- rstats %>% \n  group_by(screen_name) %>% \n  mutate(favorite_count_avg = mean(favorite_count),\n         retweet_count_avg = mean(retweet_count)) %>% \n  filter(row_number() == 1) %>% \n  select(screen_name, followers_count, favorite_count_avg, retweet_count_avg) %>% \n  filter(favorite_count_avg <= 20, retweet_count_avg <= 20)\n\n\nCheck for missing values\n\nNone of these columns have NA values, so I don’t need to worry about removing or estimating missing values.\n\nScaling the data (scale turns our dataset into a matrix)\n\n\nrstats_clustering <- rstats_clustering %>% \n  as_data_frame() %>% \n  remove_rownames() %>% \n  column_to_rownames(var = \"screen_name\") %>% \n  scale()\n\nNow, I am done with data preparation and are ready to run the K-Means Algorithm… Not so fast. I need to determine the optimal number of clusters beforehand.\nElbow & Silhouette Method (Code borrowed from DataCamp class: “Cluster Analysis in R”)\n\n# Set seed\nset.seed(42)\n\n# Use map_dbl to run many models with varying value of k (centers)\nrstats_tot_withinss <- map_dbl(1:10,  function(k){\n  rstats_kmeans <- kmeans(rstats_clustering, centers = k)\n  rstats_kmeans$tot.withinss\n})\n\n# Generate a data frame containing both k and tot_withinss\nelbow_df <- data.frame(\n  k = 1:10 ,\n  tot_withinss = rstats_tot_withinss\n)\n\n# Plot elbow plot\nggplot(elbow_df, aes(x = k, y = tot_withinss)) +\n  geom_line() +\n  scale_x_continuous(breaks = 1:10) +\n  labs(x = \"K\",\n       y = \"Total within-cluster sum of squares\") +\n  ggtitle(\"Elbow Method\",\n          subtitle = \"Since there is no definite elbow, let's use Silhouette Method\")\n\n\n\n\nSilhouette Method\n\n# Use map_dbl to run many models with varying value of k (centers)\nsil_width <- map_dbl(2:10,  function(k){\n  rstats_clustering_model <- pam(x = rstats_clustering, k = k)\n  rstats_clustering_model$silinfo$avg.width\n})\n\nsil_df <- data.frame(\n  k = 2:10,\n  sil_width = sil_width\n)\n\nggplot(sil_df, aes(x = k, y = sil_width)) +\n  geom_line() +\n  scale_x_continuous(breaks = 2:10) +\n  labs(x = \"K\",\n       y = \"Average Silhouette Widths\") +\n  ggtitle(\"Optimal Number of Clusters\",\n          subtitle = \"K = 2 has the highest Silhouette Score\")\n\n\n\n\nFinally, let’s run the K-Means Algorithm with K = 2.\n\nrstats_clustering_kmeans <- kmeans(rstats_clustering,\n                                   centers = 2,\n                                   nstart = 25)\n\nNow let’s visualize our clustering analysis using a scatterplot of Retweets vs Likes with points colored by cluster. The plotly package allows users to hover over each point and view the underlying data. Let’s show the Twitter handle, average likes, average retweets and cluster assignments in this plotly graph.\n\nclustering_plot <- rstats_clustering %>% \n  as_tibble() %>% \n  mutate(cluster = rstats_clustering_kmeans$cluster,\n         screen_name = rownames(rstats_clustering)) %>% \n  ggplot(aes(x = favorite_count_avg,\n             y = retweet_count_avg,\n             color = factor(cluster),\n             text = paste('Twitter Handle: ', screen_name,\n                  '<br>Average Likes:', round(favorite_count_avg, 1), \n                  '<br>Average Retweets:', round(retweet_count_avg, 1),\n                  '<br>Cluster:', cluster))) +\n  geom_point(alpha = 0.3) +\n  labs(x = \"Average Likes\",\n       y = \"Average Retweets\",\n       color = \"Cluster\"\n       ) +\n  ggtitle(\"K-Means (K=2) Clustering of Twitter Screen Names\")\n  \nggplotly(clustering_plot, tooltip = \"text\")"
  },
  {
    "objectID": "posts/analyzing-twitter-accounts/index.html#conclusion",
    "href": "posts/analyzing-twitter-accounts/index.html#conclusion",
    "title": "Analyzing Twitter Accounts",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, I imported the #rstats tweets dataset, prepared it for cluster analysis, determined the optimal number of clusters, and visualized those clusters with plotly. The interactive graph clearly shows two distinct clusters of twitter accounts according to average retweets, average likes, and number of followers.\nEven though the K-Means algorithm isn’t a perfect algorithm, it can be very useful for exploratory data analysis. It divides twitter accounts into two groups according to my “popularity measure” and allows for further analysis one group (Cluster 1 or Cluster 2) at a time.\nThank you for reading!"
  },
  {
    "objectID": "posts/wine-quality-prediction/index.html",
    "href": "posts/wine-quality-prediction/index.html",
    "title": "Wine Quality Prediction",
    "section": "",
    "text": "On June 14th, 2018, I participated in a Data Hackathon as part of the REU program at George Mason University. It was my first ever hackathon and I was excited to finally participate in one. It lasted approximately 4 hours, from 10am to 2pm. Our team, consisting of three undergraduate students, worked with the famous Wine Quality dataset, which is hosted by University of California Irvine’s Center for Machine Learning and Intelligent Systems. The goal of the hackathon was to accurately predict the Quality variable (“Good”= 1 or “Bad” = 0)"
  },
  {
    "objectID": "posts/wine-quality-prediction/index.html#dataset",
    "href": "posts/wine-quality-prediction/index.html#dataset",
    "title": "Wine Quality Prediction",
    "section": "Dataset",
    "text": "Dataset\nI first import the dataset and observe it.\n\n# Load tidyverse and caret package\nlibrary(tidyverse)\nlibrary(caret)\n\n# Import training / test data\nwine_train <- read_csv(\"wine_train.csv\")\nwine_test <- read_csv(\"wine_test.csv\")\n\n\nglimpse(wine_train)\n\nRows: 799\nColumns: 12\n$ `fixed acidity`        <dbl> 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7…\n$ `volatile acidity`     <dbl> 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600…\n$ `citric acid`          <dbl> 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00,…\n$ `residual sugar`       <dbl> 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.…\n$ chlorides              <dbl> 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069…\n$ `free sulfur dioxide`  <dbl> 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, …\n$ `total sulfur dioxide` <dbl> 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 10…\n$ density                <dbl> 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978,…\n$ pH                     <dbl> 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39,…\n$ sulphates              <dbl> 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47,…\n$ alcohol                <dbl> 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 1…\n$ Quality                <chr> \"B\", \"B\", \"B\", \"G\", \"B\", \"B\", \"B\", \"G\", \"G\", \"B…\n\nglimpse(wine_test)\n\nRows: 800\nColumns: 11\n$ `fixed acidity`        <dbl> 9.4, 7.2, 8.6, 5.1, 7.7, 8.4, 8.2, 8.4, 8.2, 7.…\n$ `volatile acidity`     <dbl> 0.500, 0.610, 0.550, 0.585, 0.560, 0.520, 0.280…\n$ `citric acid`          <dbl> 0.34, 0.08, 0.09, 0.00, 0.08, 0.22, 0.40, 0.39,…\n$ `residual sugar`       <dbl> 3.60, 4.00, 3.30, 1.70, 2.50, 2.70, 2.40, 2.00,…\n$ chlorides              <dbl> 0.082, 0.082, 0.068, 0.044, 0.114, 0.084, 0.052…\n$ `free sulfur dioxide`  <dbl> 5, 26, 8, 14, 14, 4, 4, 4, 4, 4, 4, 4, 7, 20, 4…\n$ `total sulfur dioxide` <dbl> 14, 108, 17, 86, 46, 18, 10, 10, 10, 12, 15, 14…\n$ density                <dbl> 0.99870, 0.99641, 0.99735, 0.99264, 0.99710, 0.…\n$ pH                     <dbl> 3.29, 3.25, 3.23, 3.56, 3.24, 3.26, 3.33, 3.27,…\n$ sulphates              <dbl> 0.52, 0.51, 0.44, 0.94, 0.66, 0.57, 0.70, 0.71,…\n$ alcohol                <dbl> 10.7, 9.4, 10.0, 12.9, 9.6, 9.9, 12.8, 12.5, 12…\n\n\nTraining data has 799 observations and 12 variables, including the target variable, Quality, while the testing data has 800 observations and exactly the same attributes except Quality."
  },
  {
    "objectID": "posts/wine-quality-prediction/index.html#data-manipulation",
    "href": "posts/wine-quality-prediction/index.html#data-manipulation",
    "title": "Wine Quality Prediction",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\n# Change columns names- Take out single quotations and underscores from names \nnames(wine_train) <- gsub(\"'\", '', names(wine_train))\nnames(wine_train) <- gsub(\" \", \"_\", names(wine_train))\nnames(wine_train)\n\n [1] \"fixed_acidity\"        \"volatile_acidity\"     \"citric_acid\"         \n [4] \"residual_sugar\"       \"chlorides\"            \"free_sulfur_dioxide\" \n [7] \"total_sulfur_dioxide\" \"density\"              \"pH\"                  \n[10] \"sulphates\"            \"alcohol\"              \"Quality\"             \n\nnames(wine_test) <- gsub(\"'\", '', names(wine_test))\nnames(wine_test) <- gsub(\" \", \"_\", names(wine_test))\nnames(wine_test)\n\n [1] \"fixed_acidity\"        \"volatile_acidity\"     \"citric_acid\"         \n [4] \"residual_sugar\"       \"chlorides\"            \"free_sulfur_dioxide\" \n [7] \"total_sulfur_dioxide\" \"density\"              \"pH\"                  \n[10] \"sulphates\"            \"alcohol\"             \n\n\n\n# Change values in Quality column: \"B\" = 0 & \"G\" = 1\nwine_train <- wine_train %>% \n  mutate(Quality = ifelse(Quality == \"B\", 0, 1))\n\n# Observe number of 0s and 1s\ntable(wine_train$Quality)\n\n\n  0   1 \n425 374"
  },
  {
    "objectID": "posts/wine-quality-prediction/index.html#feature-selection",
    "href": "posts/wine-quality-prediction/index.html#feature-selection",
    "title": "Wine Quality Prediction",
    "section": "Feature Selection",
    "text": "Feature Selection\nI first wanted to select the relevant and useful features by means of feature selection in the caret package, a popular R package for statistical machine learning. This tutorial got me started: https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/\n\n# Feature Selection #1\nset.seed(7) # Bring me luck\ntrain_cor <- cor(wine_train[, -length(names(wine_train))])\n\n# summarize the correlation matrix\nprint(train_cor)\n\n                     fixed_acidity volatile_acidity citric_acid residual_sugar\nfixed_acidity         1.0000000000      -0.30241919  0.69369727     0.17320660\nvolatile_acidity     -0.3024191923       1.00000000 -0.54708405    -0.03046016\ncitric_acid           0.6936972747      -0.54708405  1.00000000     0.13322604\nresidual_sugar        0.1732066025      -0.03046016  0.13322604     1.00000000\nchlorides            -0.0003612805      -0.01426396  0.19704211    -0.02939710\nfree_sulfur_dioxide  -0.1562296716       0.03287532 -0.04284979     0.17117834\ntotal_sulfur_dioxide -0.2105196234       0.08831363 -0.01356392     0.14129767\ndensity               0.7293551024      -0.13418708  0.44405907     0.39166663\npH                   -0.6865541686       0.26154512 -0.56343133    -0.05858156\nsulphates             0.1618399522      -0.26807580  0.28944979     0.02657419\nalcohol               0.1261203673      -0.08160458  0.18895405     0.19244357\n                         chlorides free_sulfur_dioxide total_sulfur_dioxide\nfixed_acidity        -0.0003612805        -0.156229672         -0.210519623\nvolatile_acidity     -0.0142639627         0.032875319          0.088313629\ncitric_acid           0.1970421057        -0.042849793         -0.013563916\nresidual_sugar       -0.0293970980         0.171178339          0.141297672\nchlorides             1.0000000000         0.001938843          0.022849048\nfree_sulfur_dioxide   0.0019388434         1.000000000          0.730655240\ntotal_sulfur_dioxide  0.0228490477         0.730655240          1.000000000\ndensity               0.0754952020        -0.033797520         -0.085640019\npH                   -0.2466119148         0.086088169          0.009654689\nsulphates             0.4123789331         0.050006477          0.054326009\nalcohol              -0.1500365030        -0.019738532         -0.112954461\n                         density           pH   sulphates     alcohol\nfixed_acidity         0.72935510 -0.686554169  0.16183995  0.12612037\nvolatile_acidity     -0.13418708  0.261545116 -0.26807580 -0.08160458\ncitric_acid           0.44405907 -0.563431327  0.28944979  0.18895405\nresidual_sugar        0.39166663 -0.058581563  0.02657419  0.19244357\nchlorides             0.07549520 -0.246611915  0.41237893 -0.15003650\nfree_sulfur_dioxide  -0.03379752  0.086088169  0.05000648 -0.01973853\ntotal_sulfur_dioxide -0.08564002  0.009654689  0.05432601 -0.11295446\ndensity               1.00000000 -0.379103361  0.13497642 -0.18361374\npH                   -0.37910336  1.000000000 -0.28483760  0.12470433\nsulphates             0.13497642 -0.284837597  1.00000000  0.09629489\nalcohol              -0.18361374  0.124704335  0.09629489  1.00000000\n\n# find attributes that are highly corrected (ideally >0.75)\nhigh_cor <- findCorrelation(train_cor, cutoff=0.5)\n\n# print indexes of highly correlated attributes\nprint(high_cor)\n\n[1] 1 3 7\n\n\n\nIndex 1 = fixed_acidity\nIndex 2 = citric_acid\nIndex 3 = total_sulfur_dioxide"
  },
  {
    "objectID": "posts/wine-quality-prediction/index.html#model-fitting",
    "href": "posts/wine-quality-prediction/index.html#model-fitting",
    "title": "Wine Quality Prediction",
    "section": "Model Fitting",
    "text": "Model Fitting\nSince fixed_acidity, citric_acid and total_sulfur_dioxide are highly correlated (redundant), I only used one of these features (total_sulfur_dioxide) and disposed of the two redundant ones (fixed_acidity, citric_acid). At this point, I formulated a hypothesis: a model without redundant features performs better than a model with redundant features. Let’s find out if this is true.\nSince the target variable is binary, I fit a logistic regression.\n\n# Logistic Regression\nwine_train$Quality <- factor(wine_train$Quality, levels = c(0, 1))\ntrain_log <- createDataPartition(wine_train$Quality, p=0.6, list=FALSE)\ntraining <- wine_train[train_log, ]\ntesting <- wine_train[ -train_log, ]\n\n# Hypothesis: Try logistic regression with all the predictor variables\nmod_log <- train(Quality ~ .,  data=training, method=\"glm\", family=\"binomial\")\n\nexp(coef(mod_log$finalModel))\n\n         (Intercept)        fixed_acidity     volatile_acidity \n        5.228352e+52         1.362305e+00         4.014862e-02 \n         citric_acid       residual_sugar            chlorides \n        1.273078e-01         1.032885e+00         2.292574e-01 \n free_sulfur_dioxide total_sulfur_dioxide              density \n        1.047235e+00         9.684034e-01         1.807645e-58 \n                  pH            sulphates              alcohol \n        1.667995e+00         1.355519e+01         2.251100e+00 \n\npred <- predict(mod_log, newdata=testing)\naccuracy <- table(pred, testing$Quality)\nsum(diag(accuracy))/sum(accuracy)\n\n[1] 0.7147335\n\n# Hypothesis: Try logistic regression without the redundant variables\n# Try logistic regression without highly correlated variables\nmod_log_2 <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol,  data=training, method=\"glm\", family=\"binomial\")\n\npred_2 <- predict(mod_log_2, newdata = testing)\naccuracy_2 <- table(pred_2, testing$Quality)\nsum(diag(accuracy_2))/sum(accuracy_2)\n\n[1] 0.7053292\n\n\nThe first hypothesis yields an accuracy rate of 71.5%! while the first hypothesis yields 70.8%! Apparently, including all the variables yields higher accuracy."
  },
  {
    "objectID": "posts/wine-quality-prediction/index.html#cross-validation",
    "href": "posts/wine-quality-prediction/index.html#cross-validation",
    "title": "Wine Quality Prediction",
    "section": "Cross Validation",
    "text": "Cross Validation\nAt this point, I looked on the tutorial page for the caret package to learn how to cross validate. I learned about trainControl, a function “used to specify the type of resampling”. The parameter, method, specifies repeatedcv, which stands for repeated cross validation.\n\n# Cross validation on the second model where I took out the redundant variables\nctrl <- trainControl(method = \"repeatedcv\", repeats = 10)\n\n# Train logistic regression model\nmod_log_2_ctrl <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                     total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                     trControl = ctrl, method=\"glm\", family=\"binomial\")\npred_2_ctrl <- predict(mod_log_2, newdata = testing)\naccuracy_2_ctrl <- table(pred_2, testing$Quality)\nsum(diag(accuracy_2_ctrl))/sum(accuracy_2_ctrl)\n\n[1] 0.7053292\n\n\nI got the same accuracy, which means that I didn’t use cross validation properly…I’ll have to learn more."
  },
  {
    "objectID": "posts/wine-quality-prediction/index.html#advanced-models",
    "href": "posts/wine-quality-prediction/index.html#advanced-models",
    "title": "Wine Quality Prediction",
    "section": "Advanced Models",
    "text": "Advanced Models\nIn an effort to achieve a higher accuracy score, I looked for more accurate and powerful models, such as XgBoost, Random Forests, etc.\n\n# XgBoost -----------------------------------------------------------------\nmod_xgboost <- train(Quality ~ ., data=training, \n      trControl = ctrl, method=\"xgbTree\", family=\"binomial\")\npred_xgboost <- predict(mod_xgboost, newdata = testing)\nacc_xgboost <- table(pred_xgboost, testing$Quality)\nsum(diag(acc_xgboost))/sum(acc_xgboost)\n# 70.8%\n\n# Random Forest-----------------------------------------------------------------\nmod_rf <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                  total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                trControl = ctrl, method=\"rf\", family=\"binomial\")\npred_rf <- predict(mod_rf, newdata = testing)\nacc_rf <- table(pred_rf, testing$Quality)\nsum(diag(acc_rf)) / sum(acc_rf)\n# 80.6% is an improvement!\n\n# Logit Boost-----------------------------------------------------------------\nmod_logit <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                  total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                trControl = ctrl, method=\"LogitBoost\")\npred_logit <- predict(mod_logit, newdata = testing)\nacc_logit <- table(pred_logit, testing$Quality)\nsum(diag(acc_logit)) / sum(acc_logit)\n# 72.1%\n\n# svmRadial-----------------------------------------------------------------\nmod_svm <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                     total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                   trControl = ctrl, method=\"svmRadial\")\npred_svm <- predict(mod_svm, newdata = testing)\nacc_svm <- table(pred_svm, testing$Quality)\nsum(diag(acc_svm)) / sum(acc_svm)\n# 75.2%\n\n# LMT-----------------------------------------------------------------\nmod_svm_linear <- train(Quality ~ volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide +\n                   total_sulfur_dioxide + density + pH + sulphates + alcohol, data=training, \n                 trControl = ctrl, method=\"svmLinearWeights2\")\npred_svm_linear <- predict(mod_svm_linear, newdata = testing)\nacc_svm_linear <- table(pred_svm_linear, testing$Quality)\nsum(diag(acc_svm_linear)) / sum(acc_svm_linear)"
  },
  {
    "objectID": "posts/wine-quality-prediction/index.html#conclusion",
    "href": "posts/wine-quality-prediction/index.html#conclusion",
    "title": "Wine Quality Prediction",
    "section": "Conclusion",
    "text": "Conclusion\nI learned about a totally new field in machine learning. Importantly, is very interesting and motivating since coming up with machine learning models feels like creating and refining a crystal ball that shows the future. In the future, I plan on reading through this comprehensive tutorial of the caret package, take machine learning courses on DataCamp, and hope to learn from the mistakes I made during this hackathon."
  },
  {
    "objectID": "posts/difference-regular-season-playoffs/index.html",
    "href": "posts/difference-regular-season-playoffs/index.html",
    "title": "Quantifying Differences between the Regular Season and Playoffs using Survival Analysis",
    "section": "",
    "text": "From a casual fan’s perspective, the intensity traditionally ramps up in the playoffs because teams are closer to the grand prize, the Stanley Cup. Fans are hyped up by the storylines and rivalries for every series, and so each event feels all the more momentous. So, how different are the rates of goals, shots, or hits from the regular season to the playoffs? Does the fact that a game is played during the playoffs change these rates significantly? Which rates don’t change that much?"
  },
  {
    "objectID": "posts/difference-regular-season-playoffs/index.html#methods",
    "href": "posts/difference-regular-season-playoffs/index.html#methods",
    "title": "Quantifying Differences between the Regular Season and Playoffs using Survival Analysis",
    "section": "Methods",
    "text": "Methods\nThe rates of events such as goals or shots follow a Poisson process, which counts the rate of events happening in a certain time period, such as a day or a week. One of the key assumptions of Poisson processes is that interarrival times, the time between events, follow an exponential distribution. Some real world examples of Poisson processes include number of car accidents in an area or the requests on a web server.\nSince Poisson processes do not account for the event not occuring in the duration of a game (ex.A 2-0 game where the losing team doesn’t score), I pivoted my focus to survival analysis. According to this Cross Validated post, in survival analysis, the response variable is the time that has elapsed between events. Importantly, survival analysis deals with censoring-we might have incomplete information where the event of interest doesn’t occur in the duration of the game. I consider the treatment variable to be whether the game is played during the regular season or the playoffs. Also, I account for different situations in the game: 5-on-5 and power play (PP) situations. PP includes both 5 on 4 and 5 on 3 situations and accounts for roughly 25% of the time in my dataset. 5-on-5 situations make up 48% of the data.\n\nCox Proportional Hazards Model and Hazard Ratio\n\nCox Proportional Hazards Model is a popular regression technique in Survival Analysis, where the measure of interest is hazard ratios.\nA hazard ratio greater than one indicates that as we move from the regular season to the playoffs, there is an increase in the event of interest.\nA hazard ratio smaller than one indicates that as we move from the regular season to the playoffs, there is a decrease in the event of interest.\nThe article contains scatterplots of hazard ratios by event types that contain horizontal lines at a hazard ratio of 1, which signifies a constant rate between the regular season and playoffs.\nIf the confidence intervals for hazard ratios include 1.0, the difference between the regular season and the playoffs is not significant.\n\n\n\nSurvival Curves\n\n\n\n\nThe survival curve is another visualization that shows the difference in rates between regular season and playoffs, but the curve uses a log-rank test to see if the difference is significant.\nI think of the relationship between the plots of hazard ratios and survival curves using a log-rank test as similar to that between confidence intervals (a.k.a hazard ratios) and hypothesis testing (a.k.a log-rank test). A one-to-one correspondence.\nThe graphs display two curves, one for playoffs and the other for the regular season.\nIn our graphs, the x-axis shows time during the game and the y-axis represents survival probability, which is the probability of our event not occuring.\nFor example, if our event is goals, the survival probability equals the probability of not scoring. As a result, the lower curve signifies higher rate of event.\nIf a curve decreases faster (curve is steeper), that means we are more likely to observe the event of interest in the early stages of a game.\n\n\n\nTools and Resources\nMy main tool for organizing the data is the tidyverse, an opinionated collection of R packages that share an underlying design philosophy, grammar, and data structures. For the survival analysis, I used additional R packages, including survival, survminer, ggfortify, and rms.\nFor additional research into the theory behind Survival Analysis, have a look at this blog post by Dustin Tran, research scientist at Google Brain. For a tutorial of Survival Analysis in R, click here. Alboukadel is the author of the survminer R package, which analyzes and draws patient’s survival curves."
  },
  {
    "objectID": "posts/difference-regular-season-playoffs/index.html#data",
    "href": "posts/difference-regular-season-playoffs/index.html#data",
    "title": "Quantifying Differences between the Regular Season and Playoffs using Survival Analysis",
    "section": "Data",
    "text": "Data\n\nI will be using Play by Play (PBP) data from 2007-2008 to 2017-2018 NHL seasons provided by Emmanuel Perry, creator of Corsica Hockey. The dataset contains 56 variables and over 13 million rows. Noteworthy variables I’ll be using for my analysis include Session (P for Playoffs and R for Regular Season), Event Type(Goals, Hits, Blocked Shots, Penalties, etc), Game Strength State (5v5, 4v5, 5v4, etc), and Game Seconds (Time of event during game in seconds).\n\nI wrangled the data into a format for survival analysis. Again, Session describes whether game was held during regular season(R) or playoffs(P). Event Type specifies whether event of interest (in this case, goals) happened (1). A value of 0 for Event Type refers to the end of the game. Time Difference is the survival time in minutes, or in this case the time intervals between goals."
  },
  {
    "objectID": "posts/difference-regular-season-playoffs/index.html#plot-of-hazard-ratios",
    "href": "posts/difference-regular-season-playoffs/index.html#plot-of-hazard-ratios",
    "title": "Quantifying Differences between the Regular Season and Playoffs using Survival Analysis",
    "section": "Plot of Hazard Ratios",
    "text": "Plot of Hazard Ratios\n\nFirst, let’s look at the results for goals. The hazard ratio is 0.910, meaning that as we move from regular season to the playoffs, goals decrease by approximately 9 percent in 5-on-5 situations. This matches what prior work has shown: a Globe and Mail piece by James Mirtle reveals that the number of shots on goal, shot attempts and the amount of time spent on the power play are similar between the regular season and playoffs. However, the rate of goals drops because save percentage is higher in the playoffs.\nThis makes intuitive sense. In the playoffs, teams have better goaltending since backups no longer play games. Furthermore, teams that deeper into the playoffs tend to have higher quality goaltenders, who are likely to have a higher save percentage than regular season league average. In truth, my analysis reveals that this change in saving behavior makes goal scoring decrease by almost 10% in the playoffs.\nThe second result is the hazard ratio for hits, 1.319, signifying an increase of 32% in hits from regular season to playoffs in 5-on-5 situations. As the graph shows, the magnitude of the increase in hits greatly outweighs that of other events. Evidently, the playoffs show a certain level of violence stemming from referees calling less penalties in the playoffs, as described by Chris Peters in a CBS Sports article.\nThe hazard ratio for blocked shots is 1.077, illustrating that as we move from regular season to the playoffs, blocks increase by 7.7% in 5-on-5 situations. As mentioned before, postseason hockey is all about chasing the grand prize, the Stanley Cup. This Associated Press article reveals that players display dedication and sacrifice to attain the ultimate prize: “It’s something that there’s a high desperation level come playoffs and everybody’s doing it,” said Ian Cole, who has 31 blocks in nine games.\nLastly, let’s observe coach’s challenges, a recent addition to the NHL rulebook. The hazard ratio is 0.916. In other words, coach’s challenge decreases by 8.5% in 5-on-5 situations from the regular season to playoffs. During postseason hockey, the coaches are more conservative since they don’t want to lose the opportunity to call a timeout and draw up a play in the last minutes of the game. Notably, the confidence interval for the hazard ratio of coach’s challenges is extremely wide, demonstrating that this change is not statistically significant. Thus, I would caution the reader from making a conclusion whenever we have this wide of a confidence interval for any events.\n\nOn the power play, the hazard ratio for goals is 1.022, meaning that as we move from regular season to the playoffs, goals change by a factor of 1.022, or increase by 2%. This result is validated by the fact that the top 16 teams in the league play postseason hockey, where only the best players elevate their game to compete for the Stanley Cup. The offensively gifted players, such as Sidney Crosby, Alexander Ovechkin, and Steven Stamkos, are lethal on the power play and convert chances at a higher rate than your average NHLer.\nThe distinction between the hazard ratio for goals at 5-on-5 (0.910) and on the power play (1.022) is worth discussing. At 5-on-5, I’d argue goalies essentially have five defenders protecting the net in different ways. Forwards are covering the points and fighting in puck battles along the boards, and defensemen are focused on guarding the area around the net by preventing forwards from screening the goalie and clearing rebounds. As a result, top notch goalies can perform better in the playoffs, leading to a decrease in the rate of goals. On the other hand, when defending a power play, goalies have one less player to help out. With the extra space gained from the man advantage, teams on the power play can more freely deploy their offensive weapons. In the playoffs, only the best remain and thus, we see a higher rate of goals.\nImportantly, the change in rates of power play goals and shots between the regular season and playoffs is not statistically significant since the confidence intervals for both hazard ratios include 1.0. On the other hand, the change in rates of 5-on-5 goals and shots are statistically significant since the confidence intervals do not include hazard ratios of 1.0.\nThe hazard ratios of hits in 5v5 and PP situations are very similar (1.319 and 1.324). This reveals that the change in rate of hits doesn’t depend on game situations. Players will ramp up their game level intensity during the playoffs and play more violently in both 5-on-5 situations and on power plays. This creates for a playoff atmosphere that NHL fans all love and entails huge swings in momentum and an increase in injuries. After the playoffs, few players come out unscathed.\nLet’s finally look at coach’s challenges. This event displays a wide confidence interval, similar to the confidence interval for 5-on-5 situations. Statistically, this can be interpreted as a consequence of small sample size or high standard deviation. This is due to two factors: the league only introduced coach’s challenges in 2015-2016 and they don’t occur regularly during games, certainly not as regularly as shots or hits. The former means that the Play by Play data consists mostly of seasons played before the league implemented this rule. The latter is partly due to a recent change in NHL Rule for Failed Offside Challenges: “the team that issued the challenge shall be assessed a minor penalty for delaying the game.” NHL commissioner Gary Bettman states: “We’re in effect trying to discourage using the coach’s challenge on offside unless you’re really 100 per cent certain that you’re going to win because it was a blown call,” said NHL Commissioner Gary Bettman. “The coach’s challenge was really intended to focus on glaring errors. And by imposing a two-minute penalty if you’re wrong, it should limit the number of challenges to those instances where there’s a glaring error.”"
  },
  {
    "objectID": "posts/difference-regular-season-playoffs/index.html#v5-survival-curve",
    "href": "posts/difference-regular-season-playoffs/index.html#v5-survival-curve",
    "title": "Quantifying Differences between the Regular Season and Playoffs using Survival Analysis",
    "section": "5v5 Survival Curve",
    "text": "5v5 Survival Curve\n\n\nLegend\n\nRed curve represents playoffs\nSky bluish curve represents the regular season.\n\n\n\nAnalysis\n\nThe first result is goals, where the log-rank test reveals the difference in curves is significant.\nThere are higher rates of goals in the regular season as its survival curve is lower than the playoff survival curve.\nThe graphs for hits and shots contain steeper survival curves, signaling that we are likely to observe a hit earlier in the game than goals, whose survival curves is more rounded.\nThis matches an apparent observation of hockey games; goals rarely, if ever, precede hits or shots.\nFuture work would be to analyze factors leading to difference in goal-scoring rates and shot rates. Shot angles, shot distance, or quality of goaltending are possible candidates.\nThe coach’s challenge survival curve during the playoffs looks significantly different.\nThe rate of coach’s challenge during playoffs is noticeably lower during later part of 2nd period and most of the 3rd period."
  },
  {
    "objectID": "posts/difference-regular-season-playoffs/index.html#pp-survival-curve",
    "href": "posts/difference-regular-season-playoffs/index.html#pp-survival-curve",
    "title": "Quantifying Differences between the Regular Season and Playoffs using Survival Analysis",
    "section": "PP Survival Curve",
    "text": "PP Survival Curve\n\n\nLegend\n\nRed curve represents playoffs\nSky bluish curve represents the regular season.\n\n\n\nAnalysis\n\nOn the power play, the goal survival curves look slightly different, but the log-rank test indicates that they are not statistically different.\nThis confirms the hazard ratio of goals, whose confidence interval includes 1.0.\nHits survival curves are the most different out of all the survival curves and demonstrate that the game level intensity jumps alot.\nThe blocked shot survival curves tell a similar story: There is higher attrition and risk of injuries in playoffs due to players putting their bodies on the line to win.\nIn terms of giveaways, the survival curves reveal that playoffs have higher rates of giveaways than regular season.\nFatigue and faster game tempo lead to more giveaways and exciting hockey for fans. Not so much for coaches."
  },
  {
    "objectID": "posts/difference-regular-season-playoffs/index.html#key-takeaways",
    "href": "posts/difference-regular-season-playoffs/index.html#key-takeaways",
    "title": "Quantifying Differences between the Regular Season and Playoffs using Survival Analysis",
    "section": "Key Takeaways:",
    "text": "Key Takeaways:\n\nAs we move from regular season to the playoffs, goals decrease by approximately 9 percent in 5-on-5 situations.\nIn 5-on-5 situations, there is an increase of 32% in hits and 7.7% in blocks from regular season to playoffs.\nOn the power play, goals increase by 2% from the regular season to the playoffs\nImportantly, the change in rates of power play goals and shots between the regular season and playoffs is not statistically significant,\nOn the other hand, the change in rates of 5-on-5 goals and shots are statistically significant.\nThe change in rate of hits doesn’t depend on game situations.\nThe steepness of survival curves show that goals rarely, if ever, precede hits or shots\nPower play survival curves for hits are the most different out of all the survival curves.\n\nThe author would like to thank Sam Ventura for the original idea for this article and feedback and Corey Sznajder and Evan Oppenheimer for reading over the initial version of this article.\nCode"
  },
  {
    "objectID": "posts/animating-epv-in-the-nba-extended/index.html",
    "href": "posts/animating-epv-in-the-nba-extended/index.html",
    "title": "Animating Expected Possession Value in the NBA Extended",
    "section": "",
    "text": "This is the second post about EPV. Click here for the first post."
  },
  {
    "objectID": "posts/animating-epv-in-the-nba-extended/index.html#introduction",
    "href": "posts/animating-epv-in-the-nba-extended/index.html#introduction",
    "title": "Animating Expected Possession Value in the NBA Extended",
    "section": "Introduction",
    "text": "Introduction\nIn a MIT Sloan Sports Analytics Conference Research Paper, Cervone, D’Amour, Bornn, and Goldsberry create a metric called Expected Possession Value, EPV. They use optical player tracking data to calculate the number of points the offense is expected to score by the end of the possession in real time, given everything we know now. In other words, EPV of a possession is the weighted average of the outcomes of all future paths that the possession could take (Cervone et al.)\nThe optical player tracking data is generated with STATS SPORTVU, a six-camera system installed in basketball arenas to track the real-time positions of players and the ball 25 times per second.\n\n\n\nAbove: Demo of STATS SportVU\n\n\nThe model breaks down a possession into discrete (macrotransitions) and continuous actions (microtransitions). Macrotransitions include passes, shots, and turnovers whereas microtransitions are defined as every other movement that players make with the ball.\nMathematically,\n\nA snapshot of a single possession in a game between the Spurs and Thunder illustrates the practicality of EPV:\n\nSince EPV is calculated in real time, the authors of the paper interpret EPV as a stock ticker. This lends itself well to an augmented graph where we have the spatial configuration of players on top and the stock ticker representation of EPV on the bottom:\n\nThe above graph is best explained by the authors:\n\nConsider the following possession from the Spurs/Cavaliers game on February 13, 2013. Down by 2 with less than 10 seconds left, the Spurs inbounded to Tony Parker, who drove to the basket, drawing defenders into the paint and leaving Kawhi Leonard wide open in the corner. Instead of attempting the tightly contested layup that would tie the game, Parker dished to a wide-open Leonard, who promptly sank the game-winning 3-pointer, almost unchallenged by the defense. While Leonard’s 3-pointer was this possession’s (and game’s) “checkmate”, we see from Figure 2 how valuable Parker was in setting it up. Parker actually dramatically increased the value of the possession twice - first by driving towards the basket, and then by passing to Leonard.\n\nI would like to extend this work by animating the above graph using gganimate and interpreting each movement during a NBA game on November 10th, 2013, between the Miami Heat vs Brooklyn Nets within the context of Expected Possession Value. Unlike a static graph, an animated graph conveys a strong, clear narrative that help teams understand their tracking data.\nA couple of notes: EPV is calculated as soon as the ball crosses the mid court line and the jersey numbers on each player are scraped from Swish Analytics. The site reflects recent jersey numbers, which means Lebron James is #23 in the animations even though he wore #6 for the Miami Heat.\nThis work was enabled by Dan Cervone’s data and code.\nThe code used in this post is stored here"
  },
  {
    "objectID": "posts/animating-epv-in-the-nba-extended/index.html#frantic-action-leading-to-terry-3-pointer",
    "href": "posts/animating-epv-in-the-nba-extended/index.html#frantic-action-leading-to-terry-3-pointer",
    "title": "Animating Expected Possession Value in the NBA Extended",
    "section": "Frantic Action leading to Terry 3-Pointer",
    "text": "Frantic Action leading to Terry 3-Pointer\n\n\nAs the ball rolls toward the sidelines, two Heat players double team Paul Pierce (#34), who dishes off to Andrei Kirilenko (#47). Then, the Nets incrementally increase the value of the possession by passing the ball twice, from Kirilenko to Andray Blatche (#0) and from Blatche to a wide open John Terry (#3), who knocks down the 3-pointer. During this sequence of events, EPV increases like a step function with each pass, resulting in a high percentage 3-point shot.\nThe previous EPV post talks about a similar play, “Frantic Action leading to Chalmers 3-Pointer”, where the ball is in no man’s land and EPV perfectly plateuas around 1.035. In this possession, there is no one around the ball. Consequently, EPV, a metric that seems to account for the proximity of players around the ball, shows absolutely no change. On the other hand, in the beginning of this possession, when the ball is rolling towards the slidelines, EPV does not perfectly plateua. The reason is, I think, that in this posession, the ball is surrounded by three players who are chasing it and EPV accounts for the proximity of these players, thus changing slightly."
  },
  {
    "objectID": "posts/animating-epv-in-the-nba-extended/index.html#lebron-james-alley-oop",
    "href": "posts/animating-epv-in-the-nba-extended/index.html#lebron-james-alley-oop",
    "title": "Animating Expected Possession Value in the NBA Extended",
    "section": "Lebron James Alley-Oop",
    "text": "Lebron James Alley-Oop\n\n\nRashard Lewis (#21) steals the pass from Pierce and dishes off to Lebron James (#23), who passes to Mario Chalmers (#6). On a fast break, Chalmers drives toward the basket with James and EPV jumps from just below 1.00 to more than 1.30, which illustrates a high value play. As Chalmers throws the ball up for a James’s alley-oop, EPV spikes to above 1.5, responding to the situation where James has a wide open opportunity to throw down 2 points. Lastly, as the ball is no longer in play, EPV plateaus, just like when the ball is in no man’s land during “Frantic Action leading to Chalmers 3-Pointer”. For the first time, we see the EPV dynamics of a fast break: the attacking team starts the possession with a EPV significantly higher than the EPV of a NBA baseline possession, 1.00."
  },
  {
    "objectID": "posts/animating-epv-in-the-nba-extended/index.html#miami-heat-set-play",
    "href": "posts/animating-epv-in-the-nba-extended/index.html#miami-heat-set-play",
    "title": "Animating Expected Possession Value in the NBA Extended",
    "section": "Miami Heat Set Play",
    "text": "Miami Heat Set Play\n\n\nThis possession looks like a set play by Erik Spoelstra and the Heat. After Chalmers passess off to James, he knows exactly what is about to happen. He sneaks out to the corner 3-point position, losing his defender, Paul Pierce. While James drives the basket, Pierce stays in the paint trying to stop James. However, James knows exactly what to do next: dish off to a wide open Chalmers for an easy 3-point shot. As James passes to Chalmers, EPV spikes from 0.96 to 1.15, signifying a very positive contribution to the offense. Furthermore, judging from Chalmers’ body language (wide open arms), he was fully expecting the ball. All in all, this set play entails many fluctuations in EPV, but I believe it is a successful one because it culminates in an optimal 3-point shot; it was taken at the highest EPV in the entire possession."
  },
  {
    "objectID": "posts/animating-epv-in-the-nba-extended/index.html#elite-passing-from-derron-williams",
    "href": "posts/animating-epv-in-the-nba-extended/index.html#elite-passing-from-derron-williams",
    "title": "Animating Expected Possession Value in the NBA Extended",
    "section": "Elite Passing from Derron Williams",
    "text": "Elite Passing from Derron Williams\n\n\nAs Kevin Garnett (#21) screens for Derron Williams (#31), EPV slightly increases, demonstrating a positive contribution. Eventually, Williams finds open space and drives the basket, pulling in another defender. This leaves Joe Johnson (#7) open for an easy 3-point shot. As Williams drives the basket and dishes off to a wide open Johnson, EPV skyrockets from 0.95 to 1.50. This immense jump in EPV proves that Williams is an elite point guard; his ability to use the screen to find open space, draw defenders, and ultimately distribute the ball into optimal space for his teammates entails a tremendous increase in EPV."
  },
  {
    "objectID": "posts/animating-epv-in-the-nba-extended/index.html#james-flies-through-the-air",
    "href": "posts/animating-epv-in-the-nba-extended/index.html#james-flies-through-the-air",
    "title": "Animating Expected Possession Value in the NBA Extended",
    "section": "James flies through the air",
    "text": "James flies through the air\n\n\nEPV is erratic as the Heat pass the ball around quite a bit in this possession. A notable change in EPV occurs when James finds a temporarily wide open Chalmers outside the 3-point line; EPV jumps from 0.95 to approximately 1.17. The defender closes in on him quickly and instead of shooting, Chalmers yields to a closely guarded Ray Allen (#34), decreasing EPV to 0.90. Allen forgoes the contested shot and finds James in the middle of the court. As James flies through the air towards the basket, EPV steadily increases to the maximum value in this possession (1.20), indicating that the Heat executed well by putting the ball in James hands."
  },
  {
    "objectID": "posts/animating-epv-in-the-nba-extended/index.html#james-incredible-offensive-ability",
    "href": "posts/animating-epv-in-the-nba-extended/index.html#james-incredible-offensive-ability",
    "title": "Animating Expected Possession Value in the NBA Extended",
    "section": "James’ incredible offensive ability",
    "text": "James’ incredible offensive ability\n\n\nNorris Cole (#30) uses the screen set by Chris Bosh (#1) to beat his defender and drive the basket. During this drive, EPV spikes up from below 1.0 to more than 1.2. As seen in the film, the play ends on an isolation with the ball in James hands. As soon as he fakes one way and goes the other way, EPV increases tremendously from 1.02 to 1.60, revealing that James one-on-one talent is exceptional. The play concludes with James scoring an easy layup, demonstrating that James made the difference in this possession in terms of EPV with his one-on-one play.\n\n\nIn this possession, not much happens until the point guard throws the ball up for a James alley-oop. Again, as James throws down the dunk, EPV catapults instantly, from approximately 1.00 to more than 1.50. EPV demonstrates Lebron’s offensive awareness and ability to exploit the space near the basket."
  },
  {
    "objectID": "posts/animating-epv-in-the-nba-extended/index.html#wades-smarts",
    "href": "posts/animating-epv-in-the-nba-extended/index.html#wades-smarts",
    "title": "Animating Expected Possession Value in the NBA Extended",
    "section": "Wade’s Smarts",
    "text": "Wade’s Smarts\n\n\nInstead of shooting a contested 3-pointer, Dwayne Wade (#3) wisely uses the screen set by his teammate and slips away from his defender, increasing EPV. Using the newly created space by the screen, Wade knocks down a 2-pointer. EPV demonstrates Wade’s offensive smarts; when Wade is closely defended, EPV plummets to under 1.00, but when Wade uses the screen to make space for himself, EPV rises to above 1.03.\nWhen ball goes to Lebron James, #23, EPV spikes up from 0.96 to more than 1.03, signifying that James is an excellent shooter. This reasoning is explained in the paper, in page 20, “our model estimates largely agree with basketball intuition… because LeBron James is a better shooter than Norris Cole, the value of his shot attempt is higher…”"
  },
  {
    "objectID": "posts/animating-epv-in-the-nba-extended/index.html#observing-boshs-shot-with-epv",
    "href": "posts/animating-epv-in-the-nba-extended/index.html#observing-boshs-shot-with-epv",
    "title": "Animating Expected Possession Value in the NBA Extended",
    "section": "Observing Bosh’s Shot with EPV",
    "text": "Observing Bosh’s Shot with EPV\n\n\nWade slowly dribbles up the court and surveys the defense’s formation. Wade seems to use the screen to go around his defender, but realizes that Bosh is wide open in the corner and passes off to James, who instantly relays to Bosh. As Wade passes to James, EPV rises marginally, but when James passes to Bosh, who is in position for a wide open 3-pointer, EPV shoots up from 1.12 to 1.62. Notably, with so much time and space, Bosh winds up for the shot leading to a decrease in EPV to 1.40. This illustrates that the more time he takes to take the shot, the lower the EPV because a defender could close down on him at any second and take away the wide open shot.\n\n\nThe most important and interesting moment during this possession is Bosh’s 2-point shot just inside the 3-point line. EPV falls off from above 1.05 to 0.98. Perhaps, EPV plummets at this time because Bosh is attempting a long range 2-point shot instead of taking a couple steps backwards and attempting a 3-pointer, which is 50% more valuable. Another explanation could be the congestion around the paint. There are four defenders around the paint- John Terry is the only defender far from the paint and defending Ray Allen and his deadly 3-point shot. Bosh has very little real estate to work with."
  },
  {
    "objectID": "posts/animating-epv-in-the-nba-extended/index.html#drawback-of-epv-defensive-details",
    "href": "posts/animating-epv-in-the-nba-extended/index.html#drawback-of-epv-defensive-details",
    "title": "Animating Expected Possession Value in the NBA Extended",
    "section": "Drawback of EPV: Defensive Details",
    "text": "Drawback of EPV: Defensive Details\n\nFirst Possession\n\n\n\n\nSecond Possession\n\n\nEPV does not account for defensive plays such as blocks. In the first possession, Bosh blocks the shot, but EPV doesn’t decrease at all. Instead, it just plateaus at 1.3 while the ball is rolling towards the sidelines. In the second possession, Pierce swats away a James layup, but EPV also just plateaus at 1.15. The reason is elaborated by the authors of EPV in their theoretical paper, “A Multiresolution Stochastic Process Model for Predicting Basketball Possession Outcomes”\n\nA number of smaller details could also be addressed…we also do not distinguish between different types of turnovers (steals, bad passes, ball out of bounds, etc.), though this is due to a technical feature of our data set. Indeed, regardless of the complexity and refinement of an EPV model, we stress that the full resolution data still omits key information, such as the positioning of players’ hands and feet, their heights when jumping, and other variables that impact basketball outcomes. As such, analyses based on EPV are best accompanied by actual game film and the insight of a basketball expert”"
  },
  {
    "objectID": "posts/animating-epv-in-the-nba-extended/index.html#conclusion",
    "href": "posts/animating-epv-in-the-nba-extended/index.html#conclusion",
    "title": "Animating Expected Possession Value in the NBA Extended",
    "section": "Conclusion",
    "text": "Conclusion\nUsing EPV to analyze plays like this blog post allows coaches and analysts to put their players in the best position to succeed, which means making plays that yield maximum EPV. Expected Possession Value confirms and quantifies our basketball intuition of a valuable play: a wide open 3-point shot, a fast break in an odd man situation, or penetrating the defense for an easy layup or jumper. However, a shortcoming of EPV is its failure to account for defensive turnovers such as a block. As a result, like this blog post, coaches and analysts should combine EPV analysis with game film for the most accurate representation of a play. I’d like to end this post with a memorable quote from the authors:\n\nThese insights, which can be reproduced for any valid NBA possession in our data set, have the potential to reshape the way we quantify players’ actions and decisions"
  },
  {
    "objectID": "posts/animating-epv-in-the-nba-extended/index.html#appendix",
    "href": "posts/animating-epv-in-the-nba-extended/index.html#appendix",
    "title": "Animating Expected Possession Value in the NBA Extended",
    "section": "Appendix",
    "text": "Appendix\nThis section contains static plots of the above EPV curves with event annotations such as “Pass”, “Assist”, or “Shot Made”. These static curves allow coaches and analysts to attribute a change in EPV to an event in the dataset with pinpoint accuracy.\n\n\nFrantic Action leading to Terry 3-Pointer\n\n\n\nLebron James Alley-Oop\n\n\n\nMiami Heat Set Play\n\n\n\nElite Passing from Derron Williams\n\n\n\nJames flies through the air\n\n\n\nJames’ incredible offensive ability\n\n\n\nJames’ Alley-oop\n\n\n\nWade’s Smarts\n\n\n\nBosh Wide Open 3-Pointer\n\n\n\nBosh Long Range 2-Pointer"
  },
  {
    "objectID": "posts/demo-nhl-play-by-play-app/index.html",
    "href": "posts/demo-nhl-play-by-play-app/index.html",
    "title": "Demo of NHL Play-by-Play App",
    "section": "",
    "text": "As part of the 1st RStudio Shiny Contest, I created the NHL Play-by-Play App. Previously, I wrote about the makings of this app (Hacking the NHL Play-by-Play App in Shiny), but I realized that I should provide an example of how this app can be used to extract information. As a result, in this post, I will be showing insights from looking at a regular season game between the Montreal Canadiens and the Toronto Maple Leafs on October 3rd, 2018."
  },
  {
    "objectID": "posts/demo-nhl-play-by-play-app/index.html#how-to-search-for-game-id",
    "href": "posts/demo-nhl-play-by-play-app/index.html#how-to-search-for-game-id",
    "title": "Demo of NHL Play-by-Play App",
    "section": "How to Search for Game ID",
    "text": "How to Search for Game ID\n\n\nSelect Season from NHL Season dropdown menu\nFind Game ID from Table\nEnter Game ID in NHL.COM GAME ID\n\n\n\nShot Chart\n\n\nMontreal Canadiens shot locations are displayed on the left half of the rink containing the Canadiens logo. Toronto Maple Leafs shot locations are displayed on the right half of the rink containing the Maple Leafs logo.\nLooking at the region between the faceoff circle and the blue line, Toronto took significantly more shots on the left side (5 vs 1), signifying that Toronto’s attack was tilted to the left-side and perhaps, Toronto’s left defensemen were more involved in the offense than Toronto’s right defensemen.\nThe Canadiens took alot of shots around the net whereas Toronto’s shots are spread out. There is one Canadiens shot outside the blue line that we can explore further using the interactive shot chart below.\n\n\n\n\nInteractive Shot Chart\n\n\nInteractive Shot chart shows information that the static shot chart doesn’t contain: distance of shot from goal, name of the shooter, players who assisted the goal (if shot is a goal), number of goals/assists a player scored, type of shot, and goal probability (probability that the shot is a goal).\nLarge circles represent goals whereas small circles represent shots\nGenerally, shots taken closer to the net and toward the middle of the ice have higher goal probability. A good example of this is Armia’s shot; it is taken outside the blue line and has a 1.6% chance of going in the net. On the other hand, Lehkonen’s goal, 4 ft from the net, has a 14.5% goal probability.\n\n\n\n\nShot Animation\n\n\nA real-time representation of the Play-By-Play data that look at two event types, shots/goals. Since both event types are directed toward the net, the animation shows all points moving towards the net.\nIn the near future with real-time rendering of gganimate and the advent of NHL tracking data, I can create a more accurate animation of the puck and players, similar to my NBA animations\n\n\n\n\nShot Distance\n\n\nHistogram of shot distances support what we learned from the shot chart: “Montreal took a lot of shots around the net whereas Toronto’s shots are spread out”\nHistogram contains vertical lines for the goal line, faceoff dots, and blue line. This helps users visualize the placement of shots on the ice as another angle to the shot chart.\n\n\n\n\nGoal Probability Animation\n\n\nIt shows the ebbs and flows of the game very well since it visualizes the momentum swings of the game.\nVertical lines represent end of periods 1,2,3\n\n\n\n\nGame Recap\n\n\nOfficial NHL.com Recap\nWatch highlights, box score, and the summary of the game."
  },
  {
    "objectID": "posts/automatic-detection-upwelling/index.html",
    "href": "posts/automatic-detection-upwelling/index.html",
    "title": "Automatic Detection Method",
    "section": "",
    "text": "The National Oceanic and Atmospheric Administration (NOAA) defines upwelling as “a process in which deep, cold water rises toward the surface”. It is initiated by wind blowing parallel to the coastline and moving warm water offshore, perpendicular to its direction. As surface water moves offshore, deeper, colder, and nutrient rich water rises up from deep sea level and replaces the warm water. “The upward movement of this deep, colder water is called upwelling” (NOAA Ocean Explorer).\n\nFigure 1: Illustration of Upwelling. As surface winds blow towards the equator, they push warm coastal water away from the coast, which brings in deeper, colder, nutrient rich water from beneath the surface to replace the warm water.\nUpwelling brings nutrient rich water to the surface and supplies nutrition necessary for biological productivity. “Coastal upwelling ecosystems like the U.S. west coast are some of the most productive ecosystems in the world and support many of the world’s most important fisheries. Although coastal upwelling regions account for only one percent of the ocean surface, they contribute roughly 50 percent of the world’s fisheries landings” (NOAA Ocean Explorer).\nLiterature on upwelling highlight changes in upwelling patterns. Bograd et al. (2009) discovered “a trend towards a later and shorter upwelling season in the northern CCLME (California Current large marine ecosystem)”. They also assert that there is “significant interannual variability in upwelling characteristics”. The spring upwelling began later, resulting in a shorter upwelling season. Studying upwelling is useful for “resource managers tasked with assessing future changes in commercially important fish and protected species populations”.\nXiu et al. (2018) looked at the effect of global warming on coastal upwelling systems. They state, “global warming will enhance land–sea temperature gradients that in turn will increase upwelling favorable winds (i.e., the Bakun hypothesis)”. Experts predict “increased upwelling intensity and duration at higher latitudes”. This will affect global fish supply as change in upwelling is a driving factor of change in marine ecosystems."
  },
  {
    "objectID": "posts/automatic-detection-upwelling/index.html#purpose",
    "href": "posts/automatic-detection-upwelling/index.html#purpose",
    "title": "Automatic Detection Method",
    "section": "2 Purpose",
    "text": "2 Purpose\nWe introduce a simple, but highly effective algorithm for detecting upwelling: an automatic detection method using satellite image data from NOAA. This method identifies the upwelling hotspots, months of strong upwelling, and the duration of upwelling. This will allow marine scientists and researchers to prepare for the effect of global warming on upwelling systems."
  },
  {
    "objectID": "posts/automatic-detection-upwelling/index.html#data",
    "href": "posts/automatic-detection-upwelling/index.html#data",
    "title": "Automatic Detection Method",
    "section": "3 Data",
    "text": "3 Data\nThe dataset, Daily Optimum Interpolation (OI) SST, comes from NOAA’s data server, ERDDAP. It contains four columns: time, latitude, longitude, and SST (sea surface temperature). SST is defined as the water temperature close to the ocean’s surface.\n\n\n\n\n\n\n\n\nFigure 2: NOAA satellite image showing sea surface temperature (SST) in Celsius off the coast of Vancouver Island and Washington. Red is high temperature (~18°C), green is medium temperature (~15°C) and blue/purple is low temperature (~13°C)\nAs seen in Figure 2, we chose to focus on latitudes [42.625°N, 52.125°N] and longitudes [229.875°W, 236.625°W] because this area avoids the Alaska current to the north and the California current to the south. Also, the east boundary crops out land, which will save memory. This area corresponds to the coast of Vancouver Island (British Columbia) and Washington."
  },
  {
    "objectID": "posts/automatic-detection-upwelling/index.html#automatic-detection-method",
    "href": "posts/automatic-detection-upwelling/index.html#automatic-detection-method",
    "title": "Automatic Detection Method",
    "section": "4 Automatic Detection Method",
    "text": "4 Automatic Detection Method\nGidhagen (1987) first uses an automatic method to define upwelling: “Upwelling was proved if an in situ measurement showed an abnormal temperature drop of at least 2°C compared to earlier and surrounding measurements”. Using this method, he found that September is a month of strong upwelling in the Swedish coastline of the Baltic Sea. “The increased rate of upwelling in September is not due to a higher number of upwellings, but rather to a longer duration”. Overall, he found “there is a strong year-to-year variation in the rate of upwellings”.\nLehmann, Myrberg, and Höflich (2012) discusses the automatic detection method: “Upwelling was detected by calculating the temperature difference for each individual pixel from the zonal mean temperature, for every pixel line.” The authors tested two temperature thresholds (2°C and 3.5°C), both of which yielded “erroneous upwelling areas detected far offshore”. Thus, they registered upwelling if it occurred within 28 km of the coast.\nDrawing inspiration from past work, we created a custom automatic detection method to algorithmically determine when and where upwelling occurs. The algorithm looks at each latitude and finds the SST of the coastal water (water closest to land) and the SST of water 2 and 3 degrees away from land (SST coast 2 and SST coast 3). Then, it computes two differences: the difference in SST between the coastal water and SST coast 2 and the difference in SST between the coastal water and SST coast 3. It checks if either of these two differences exceeds a threshold of 2.5°C. If it does, we indicate upwelling at the specific latitude. If not, we indicate non-upwelling.\n\nFigure 3: Stamen map showing upwelling locations off the coast of Vancouver Island and Washington. Red dots are placed at latitudes where we detected upwelling. Blue dots are placed 2 and 3 degrees away from the red dots.\nAs seen in Figure 3, the automatic detection method accurately portrays the location of upwelling along the coast. It corresponds well with visually looking at the SST and finding locations where the color of the SST on the coast is blue and the color of the SST on offshore waters is red. In a later section, Section 5.0.2, we will illustrate the changes in the timing and length of upwelling season from 1982 to 2020."
  },
  {
    "objectID": "posts/automatic-detection-upwelling/index.html#results",
    "href": "posts/automatic-detection-upwelling/index.html#results",
    "title": "Automatic Detection Method",
    "section": "5 Results",
    "text": "5 Results\n\n5.0.1 Relationship of the Automatic Detection Method with the Ekman Upwelling Index\nThe automatic detection method gives us an indicator of upwelling at a certain latitude. This allows us to perform logistic regression to examine the relationship between a predictor and the upwelling indicator.\nWe merged our dataset with data on the daily averages of Ekman upwelling index data at three different latitudes: 42°N, 45°N, and 48°N. This data comes from the Environmental Research Division at NOAA Fisheries - Southwest Fisheries Science Center. The Ekman upwelling index is an index of the strength of upwelling along the coast and is calculated using wind dynamics. Positive values indicate upwelling and negative values non-upwelling.\n\n\n\n\n\nFigure 4 : Line plot of monthly averages of Ekman upwelling index on the y-axis and year on the x-axis facetted by latitude (42°N, 45°N and 48°N) and upwelling months (June ~ Oct / Nov ~ May).\nAbove is a line plot of the monthly averages of Ekman upwelling index facetted by upwelling months and latitude. Upwelling months are from June to October, months known to have strong upwelling patterns. Non-upwelling months are from November to May. We observe the trend line in the non-upwelling months to be well below zero and the trend line in the upwelling months to be well above zero. This demonstrates that the upwelling index is appropriate for detecting upwelling. Below, we further examine the relationship between the upwelling labels from the automatic detection method and the upwelling index with logistic regression.\n\n\n\n\n\nFigure 5: Logistic curve of upwelling indicator variable on the y-axis and daily average of Ekman upwelling index on the x-axis at three latitudes: 42°N, 45°N, and 48°N. Also shown is histogram of the distribution of the upwelling index at upwelling (1) and non-upwelling (0).\nAbove is a logistic curve drawn on a plot of the upwelling indicator variable and daily average of Ekman upwelling index. As we go from 42°N to 45°N and from 45°N to 48°N, there seem to be less and less positive upwelling indexes at y = 1. In other words, at higher latitudes, there is less of a correlation between Ekman upwelling index and the automatic detection method. When we ran a logistic regression on two predictors, Ekman upwelling index and latitude, we observed that daily average of upwelling index has a statistically significant, positive relationship with the binary variable of upwelling (0/1).\n\n\n5.0.2 Exploration of Upwelling Percentage\n\nUpwelling percentage: Proportion of days over relevant timeframe in a figure that are categorized as upwelling by our automatic detection method.\n\n\n\n\n\n\nFigure 6: Heatmap of upwelling percentage over four decades. Red means high upwelling percentage (~50%) , blue means low upwelling percentage (~15%), and green is the average upwelling percentage (~35%). The 2020s has been omitted for lack of data.\nOverall, low latitudes have higher upwelling percentage than high latitudes. Over the four decades, upwelling percentage is highest (~50%) at 40.0°N ~ 42.5°N. Then, it decreases to around 35% at 42.5°N ~ 45.0°N. Above this latitude, upwelling percentage decreases to 15% at 45.0°N ~ 50.0°N. Notably, in the 1980s and 2010s, upwelling percentage around 35% was observed at the lower latitudes (40°N ~ 42.5°N) and higher latitudes (47.5°N ~ 50.0°N).\n\n\n\n\n\nFigure 7: Heatmap of the proportion of upwelling days in each month from 1982 to 2020. Red means high upwelling percentage (~95%) , blue means low upwelling percentage (~0%), and green is the average upwelling percentage (~50%).\nThis plot illustrates a a fairly consistent pattern of high upwelling percentage from June to October. There is extremely high upwelling percentage (~95%) in August, September, and October. On the other hand, there is low upwelling percentage (~0%) from November to May. This shows the upwelling months (upwelling percentage higher than 25%), to be from June to October and the non-upwelling months (upwelling percentage lower than 25%) from November to May.\n\n\n\n\n\nFigure 8: Heatmap of upwelling percentage facetted by latitude bins from 1982 to 2020. Red means high upwelling percentage (~95%) , blue means low upwelling percentage (~0%), and green is the average upwelling percentage (~50%). This plot segments the previous plot into four latitude bins: [40°N, 43.5°N], [43.5°N, 46.126°N], [46.125°N, 47.625°N], and [47.625°N, 50°N].\nOverall, there is extremely high upwelling percentages (~95%) during the upwelling months at the 40°N ~ 43.5°N latitude bin and lower upwelling percentages as latitude increases. In the northern latitudes, there are some years with high upwelling percentage, but they are not as common as in the southern latitudes.\n\n\n\n\n\nFigure 9: Heatmap of upwelling percentage by latitude and month from 1982 to 2020. Red means high upwelling percentage (~95%) , blue means low upwelling percentage (~0%), and green is the average upwelling percentage (~50%).\nThis plot confirms the above findings of a consistent upwelling pattern from June to October and higher upwelling percentages in the lower latitudes (40°N ~ 45°N). Interestingly, there is a glimmer of high upwelling percentage around 48.0°N from August to September. This latitude is same as the latitude where in the 1980s and 2010s, upwelling percentage was observed to be around 35%.\n\n\n5.0.3 Changes in Upwelling Phenology and Duration\n\n\n\n\n\nFigure 10: Heatmap showing the first day of upwelling off the coast of Vancouver Island and Washington. Red squares signify later upwelling days (late September) and blue squares earlier days (June). White squares indicate missing values. Heatmap showing the last day of upwelling is placed in the Appendix since it shows no clear pattern.\nThe plot above shows that the first day suggesting a coastal upwelling process by our automatic detection method was from June to October. Overall, upwelling started later for the northern latitudes (45.0°N ~ 50.0°N) than the southern latitudes (40.0°N ~ 45.0°N). There is a band of latitudes around the late 1990s and late 2000s when the first day of upwelling was late September. Also, around 48.0°N, the first day of upwelling is much earlier (June) than the surrounding latitudes, where the first day of upwelling was in August.\n\n\n\n\n\nFigure 11: Heatmap showing the duration of upwelling off the coast of Vancouver Island and Washington. Red squares indicate short upwelling duration (~10 days) and blue squares long upwelling duration (~150 days). White squares indicate missing values.\nOverall, southern latitudes (40.0°N ~ 45.0°N) show longer duration of upwelling. This is natural since the previous plot showed that in the southern latitudes, upwelling started earlier than the northern latitudes. There are some northern latitudes with short duration of upwelling. Around 50.0°N and 47.5°N, the duration of upwelling is shorter than its surrounding latitudes. In 1997, a shorter duration of upwelling was observed at 45.0°N ~ 47.5°N than surrounding years.\n\n\n5.1 Discussion\nOur automatic detection method mainly draws inspiration from Gidhagen (1987). Gidhagen determined a coastal upwelling process with “an abnormal temperature drop of at least 2°C compared to earlier and surrounding measurements”. This revealed that in the Baltic Sea, the range of the temperature drop for upwelling events is 2-10°C and a typical drop is 4-5°C. Our method uses a threshold of 2.5°C, which is optimal for reducing noise. While Gidhagen compares the current coastal temperature in two dimensions, time and location, we only look at one dimension, comparing the coastal SST to the SST that are 2 and 3 degrees away.\nEleven years later, Naidu, Ramesh Kumar, and Ramesh Babu (1999) studied monsoonal upwelling along the west and east coasts of India using monthly mean local temperature anomaly (LTA). LTA is defined as the diffference between coastal and mid-ocean sea surface temperatures (SST). Specifically, they calculate two LTAs, one on the west coast (\\(LTA_W\\)) and another on the east coast (\\(LTA_E\\)).\n\\[\\begin{align}\nLTA_W &= T_{64}-T_{\\text{west coast}} \\text{(along the same latitude belt)} \\\\\n\nLTA_E &= T_{90}-T_{\\text{east coast}} \\text{(along the same latitude belt)}\n\n\\end{align}\\]\nIn the Arabian Sea, they used “64°E (\\(T_{64}\\)) for representing the mid-ocean conditions whereas for the Bay of Bengal we used the 90°E (\\(T_{90}\\)) for the open ocean conditions”. Positive LTA values indicate coastal upwelling process.\nTen years later, Smitha et al. (2008) also used LTA as an upwelling index.\nLTA is calculated as\n\\[\\begin{align}\n\nLTA_{wc} &= T_{lon - 3}-T_{\\text{lon}} \\text{(along the south-west coast)} \\\\\n\nLTA_{kk} &= T_{lat - 3}-T_{\\text{lat}} \\text{(Off Kanyakumari)}\n\n\\end{align}\\]\n\\(T_{\\text{lat}}\\) and \\(T_{\\text{lon}}\\) refer to the coastal stations between 8.5° - 14.5° N and 76.5° - 78.5°E and \\(T_{lon - 3}\\) and \\(T_{\\text{lat} - 3}\\) refer to SST 333 km from the coast. \\(LTA_{wc}\\) and \\(LTA_{kk}\\), respectively, represent local temperature along the south-west coast and the Kanyakumari coast. The positive LTA values suggest coastal upwelling process.\nMost recently, Holmes et al. (2021) used SST differential to study upwelling in the Southeast Arabian Sea, one of the world’s important seasonal upwelling zones. For their SST data, they used the same source as this blogpost, the Daily Optimum Interpolation (OI), version 2.1 data set by the Group for High Resolution Sea Surface Temperature (GHRSST). “This data set uses Advanced Very High Resolution Radiometer (AVHRR) data, which provides accurate nearshore SST values, and interpolates to fill in gaps in the AVHRR data”. Their upwelling index is “the SST differential between nearshore and 3° longitude offshore, based on Naidu, Ramesh Kumar, and Ramesh Babu (1999) and Smitha et al. (2008).” This index is good for measuring “upwelling arising due to both remote-forcing and local wind stress.”\nHickey and Banas (2003) reveals that “In general, the strength and duration of upwelling increases to the south in the PNW”. They define strength of upwelling with wind velocity, whereas we look to upwelling percentage. Figure 8 and Figure 9 clearly confirm this finding as they illustrate upwelling percentage is much higher in the lower latitudes in the PNW, 40.0°N ~ 45.0°N, than the higher latitudes, 45.0°N ~ 50.0°N. In fact, Figure 6 shows that this has been the case over several decades since the 1980s.\nAlso, they mention that maximum upwelling happens in spring and summer. Our automatic detection method shows that the upwelling months (months with upwelling percentage > 25%) are from June to October. This means upwelling months start later in the summer and end later in the Fall than Hickey and Banas’ assertion.\nFinally, Hickey and Banas write, “the duration of coastal upwelling also decreases seasonally towards the north”. Figure 11 illustrates this point, as it shows that duration of upwelling is around 150 days at 40.0°N ~ 45.0°N, and decreases to around 100 days for northern latitudes, 45.0°N ~ 50.0°N.\nBograd et al. (2009) develop a set of indexes to understand the coastal upwelling in the California Current large marine ecosystem (CCLME). They found that coastal upwelling gets “progressively shorter with northward latitude”. In Table 1, they show that the mean duration of upwelling is 357 days at 33.0°N, 320 days at 36°N, 223 days at 42°N, and 151 days at 48°N. This finding agrees with our Figure 11, which shows northern latitudes with shorter duration of upwelling. One difference is Bograd et al only looked at data up to 2007, whereas our dataset looks up to 2020."
  },
  {
    "objectID": "posts/automatic-detection-upwelling/index.html#conclusion",
    "href": "posts/automatic-detection-upwelling/index.html#conclusion",
    "title": "Automatic Detection Method",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nUpwelling is the upward movement of deep, cold water caused by wind blowing parallel to the coastline. We designed a custom automatic detection method using SST differential to accurately identify when and where upwelling occurs. We discovered that monthly averages of Ekman upwelling index has a statistically significant relationship with the upwelling indicator, which were determined by our automatic detection method.\nUsing these indicators, we calculated the upwelling percentage, the proportion of days over relevant timeframe in a figure that are categorized as upwelling by our automatic detection method. We discovered that southern latitudes (40°N ~ 45°N) had higher upwelling percentages than northern latitudes (45°N ~ 50°N). There was extremely high upwelling percentage, close to 50%, from 40°N to 42.5°N. Also, we found the upwelling months (upwelling percentage higher than 25%) to be from June to October and the non-upwelling months from November to May. High upwelling percentage (~95%) was observed from August to October.\nWe then looked at the changes in upwelling phenology and focused on the upwelling months. Upwelling started later for northern latitudes (45.0°N to 50.0°N) around August and September. In the southern latitudes, upwelling started around July 1st. Northern latitudes had shorter duration of upwelling than the southern latitudes."
  },
  {
    "objectID": "posts/automatic-detection-upwelling/index.html#appendix",
    "href": "posts/automatic-detection-upwelling/index.html#appendix",
    "title": "Automatic Detection Method",
    "section": "7 Appendix",
    "text": "7 Appendix\n\nFigure 12: Lineplots of upwelling percentage over four decades. The 2020s has been omitted for lack of data.\n\nFigure 13: Lineplots of upwelling percentage from 1982 to 2020.\n\nFigure 14: Lineplots of upwelling percentage of each month from 1982 to 2020.\n\nFigure 15: Heatmap showing the last day of upwelling off the coast of Vancouver Island and Washington. Red squares signify later upwelling days (late September) and blue squares earlier days (June). White squares indicate missing values."
  },
  {
    "objectID": "posts/animating-epv-in-the-nba/index.html",
    "href": "posts/animating-epv-in-the-nba/index.html",
    "title": "Animating Expected Possession Value in the NBA",
    "section": "",
    "text": "In a MIT Sloan Sports Analytics Conference Research Paper, Cervone, D’Amour, Bornn, and Kirk Goldsberry create a metric called Expected Possession Value, EPV. They use optical player tracking data to calculate the number of points the offense is expected to score by the end of the possession in real time, given everything we know now. In other words, EPV of a possession is the weighted average of the outcomes of all future paths that the possession could take (Cervone et al.)\nThe model breaks down a possession into discrete (macrotransitions) and continuous actions (microtransitions). Macrotransitions include passes, shots, and turnovers whereas microtransitions are defined as every other movement that players make with the ball.\nMathematically,\n\nA snapshot of a single possession in a game between the Spurs and Thunder illustrates the practicality of EPV:\n\nSince EPV is calculated in real time, the authors of the paper interpret EPV as a stock ticker. This lends itself to an augmented graph where we have the spatial configuration of players on top and the stock ticker representation of EPV on the bottom:\n\nThe above graph is best explained by the authors:\n\nConsider the following possession from the Spurs/Cavaliers game on February 13, 2013. Down by 2 with less than 10 seconds left, the Spurs inbounded to Tony Parker, who drove to the basket, drawing defenders into the paint and leaving Kawhi Leonard wide open in the corner. Instead of attempting the tightly contested layup that would tie the game, Parker dished to a wide-open Leonard, who promptly sank the game-winning 3-pointer, almost unchallenged by the defense. While Leonard’s 3-pointer was this possession’s (and game’s) “checkmate”, we see from Figure 2 how valuable Parker was in setting it up. Parker actually dramatically increased the value of the possession twice - first by driving towards the basket, and then by passing to Leonard. \n\nI would like to extend this work by animating the above graph using gganimate and interpreting each movement during final minute of a NBA game on November 10th, 2013, between the Miami Heat vs Brooklyn Nets within the context of Expected Possession Value. Unlike a static graph, an animated graph conveys a strong, clear narrative that help teams understand their tracking data.\nBriefly, this game is played at Brooklyn in the early stages of the 2013-2014 NBA season, with Brooklyn expected to challenge Miami for a championship in the Spring. The home fans are expecting a victory as their team is up 8 points with one minute left.\nA couple of notes: EPV is calculated as soon as the ball crosses the mid court line and the jersey numbers on each player are scraped from Swish Analytics. This site reflects recent jersey numbers, which means Lebron James is #23 in the animations even though he wore #6 for the Miami Heat.\nThis work was enabled by Dan Cervone’s data and code.\nThe code used in this post is stored here"
  },
  {
    "objectID": "posts/animating-epv-in-the-nba/index.html#paul-pierce-isolation",
    "href": "posts/animating-epv-in-the-nba/index.html#paul-pierce-isolation",
    "title": "Animating Expected Possession Value in the NBA",
    "section": "Paul Pierce Isolation",
    "text": "Paul Pierce Isolation\n\n\nAt 1:13, the Nets move the ball up the court and as soon as the ball crosses the mid court line, EPV is calculated. The offense starts in an isolation formation with Paul Pierce being defended by Mario Chalmers. As Pierce dribbles toward the 3 point line, EPV steadily increases to approximately 1.00. However, as he drives the basket, EPV take a steep dive, demonstrating a negative contribution to the offense. From the moment the ball crosses the half circle, EPV has decreased from just over 1.00 all the way to 0.90. When Pierce pulls up for a 17-foot jumper near the free throw line, EPV sharply jumps to 0.93, then takes another plunge once the ball hits the back of the rim. Expected Possession Value tells us that Pierce drastically decreased the value of the possession by driving towards the basket."
  },
  {
    "objectID": "posts/animating-epv-in-the-nba/index.html#wide-open-3-point-shot-from-dwayne-wade",
    "href": "posts/animating-epv-in-the-nba/index.html#wide-open-3-point-shot-from-dwayne-wade",
    "title": "Animating Expected Possession Value in the NBA",
    "section": "Wide Open 3-point Shot from Dwayne Wade",
    "text": "Wide Open 3-point Shot from Dwayne Wade\n\n\nAt 1:03, Lebron James grabs the rebound and drives all the way to the basket. During this time, EPV drops from around 1.00 to 0.80. This resembles the previous change in EPV during Pierce’s drive towards the basket. Back to this play, instead of shooting a contested 2-point jumper or a layup, James incrementally increases the value of the possession by dishing to an open Chris Bosh at the 3- point line. Bosh wisely passes to Chalmers, a much better 3-point shooter. However, EPV decreases since Bosh was wide open wheras Chalmers faces defensive pressure from Joe Johnson. Once Chalmers dishes to Ray Allen, a prolific 3 point shooter, EPV goes down as Derron Willliams frantically runs to defend Allen. Allen makes the optimal decision and passes to Dwayne Wade in the corner. Wade is left wide open after Williams gambles on Allen shooting the three and Pierce’s unwillingness to defend. At this moment, EPV jumps by 0.1 and after Wade releases a wide open 3-pointer, shoots up to almost 1.60."
  },
  {
    "objectID": "posts/animating-epv-in-the-nba/index.html#kevin-garnett-shot-from-the-paint",
    "href": "posts/animating-epv-in-the-nba/index.html#kevin-garnett-shot-from-the-paint",
    "title": "Animating Expected Possession Value in the NBA",
    "section": "Kevin Garnett Shot from the Paint",
    "text": "Kevin Garnett Shot from the Paint\n\n\nAt 50.4 seconds, Williams slowly dribbles toward the basket before Garnett sets a pick. As Williams moves to the center of the 3-point perimeter defended by Bosh, EPV slightly increases. When Williams passes off to Garnett, who is parked just inside the free throw line, EPV greatly decreases, signifying a very negative impact on the offense. Then, Garnett double clutches and shoots a 2-point jumper in the paint, at which EPV hovers around 0.8. Ultimately, he misses the shot and the ball is eventually rebounded by James. I would have liked Garnett to return the ball to Williams, which would have entailed higher value. The reason is that Bosh realizes that Wade is outmatched by the much taller Garnett and leaves Williams to help out Wade. As a result, Williams was wide open for a 3-pointer and would have had a better chance of knocking down the 3-pointer than Garnett, who was draped by Wade and Bosh."
  },
  {
    "objectID": "posts/animating-epv-in-the-nba/index.html#frantic-action-leading-to-chalmers-3-pointer",
    "href": "posts/animating-epv-in-the-nba/index.html#frantic-action-leading-to-chalmers-3-pointer",
    "title": "Animating Expected Possession Value in the NBA",
    "section": "Frantic Action leading to Chalmers 3-Pointer",
    "text": "Frantic Action leading to Chalmers 3-Pointer\n\n\nAt 33.0 seconds, James dribbles up the court and attempts a pass, which bounces off Anderson’s head to the sidelines. While the ball is in no-man’s land, EPV plateuas around 1.035, then takes quite a significant plunge when Allen runs over to the sidelines to retrieve the ball! As Allen passes up top to Chalmers who sets up the offense, EPV climbs back up, showing the recovery of offense’s value. Interestingly, as Chalmers dribbles on the periphery of the 3-point line, EPV spikes up to around 1.075 from 1.025. As Chalmers passes off to Bosh who then passes to James, EPV plateuas each time. As James gets ready to drive the basket, EPV increases, until he dishes off to Chalmers. This offensive possession shows hectic action starting with the ball bouncing off Anderson’s head to the sidelines. As Allen recovers possession and passes off to Chalmers, both sides are in a tumultous state as they try to reset formation. One example of this is shown in the beginning of the game film, when Ray Allen is tustling with Kevin Garnett to regain formation. Fortunately for the Heat, they are rewarded with a Chalmers 3-point shot."
  },
  {
    "objectID": "posts/animating-epv-in-the-nba/index.html#conclusion",
    "href": "posts/animating-epv-in-the-nba/index.html#conclusion",
    "title": "Animating Expected Possession Value in the NBA",
    "section": "Conclusion",
    "text": "Conclusion\nThe final minute of this game revealed several insights about Expected Possession Value.\n\nDriving towards the basket with a defender entails a guaranteed decrease in EPV.\nPulling up for a shot leads to an ephemeral spike in EPV.\nEPV drastically increases once a player passes to a wide open teammate but decreases once a defender closes down on him.\nEPV plateaus when player holds onto ball without dribbling.\nEPV shows great fluctuations in loose ball situations when players deviate from offensive/defensive structure.\n\nIf you enjoyed this post, you will enjoy Animating Expected Possession Value in the NBA: Part 2"
  },
  {
    "objectID": "posts/animating-epv-in-the-nba/index.html#appendix",
    "href": "posts/animating-epv-in-the-nba/index.html#appendix",
    "title": "Animating Expected Possession Value in the NBA",
    "section": "Appendix",
    "text": "Appendix\nThis section contains static EPV curves with labels from the dataset, such as “Pass”, “Dribble”, or “Shot Made”. These plots supplement the animated EPV curves with real-time event annotations.\n\nPaul Pierce Isolation\n\n\n\nWide Open 3-point Shot from Dwayne Wade\n\n\n\nKevin Garnett Shot from the Paint\n\n\n\nFrantic Action leading to Chalmers 3-Pointer"
  },
  {
    "objectID": "posts/are-teams-getting-lucky-on-rushes/index.html",
    "href": "posts/are-teams-getting-lucky-on-rushes/index.html",
    "title": "Are Teams getting Lucky on Rushes?",
    "section": "",
    "text": "When you watch an offensive rush in hockey, do you ever wonder about the numbers behind it? For example, is the number of shots that were preceded by passes repeatable over an entire season? What about shooting percentages? If repeatable, do zones of the primary pass (the pass preceding a shot) influence this repeatability? What about rebounds and rebound shooting percentages (the goals scored from rebounds)?"
  },
  {
    "objectID": "posts/are-teams-getting-lucky-on-rushes/index.html#terminology",
    "href": "posts/are-teams-getting-lucky-on-rushes/index.html#terminology",
    "title": "Are Teams getting Lucky on Rushes?",
    "section": "Terminology",
    "text": "Terminology\nIn hockey, “odd-man rushes” is a term frequently used to refer to offensive attacks such as the above where the attacking team has more players than the defending team. In my analysis, I will be slightly deviating from this jargon and instead use “odd-player rushes”, which consist of shots that were preceded by passes and taken on breakaways, 2-on-1, 3-on-2, etc. Any shots that are not rush shots with a player advantage are categorized as all_other_shots.\nIn the later parts of this analysis, I will be using the terms, “rebound shot” and “rebound shooting percentage”. The first indicates a shot on goal following a rebound and the second is calculated as rebound goals (goals that follow rebounds) divided by rebound shots."
  },
  {
    "objectID": "posts/are-teams-getting-lucky-on-rushes/index.html#data",
    "href": "posts/are-teams-getting-lucky-on-rushes/index.html#data",
    "title": "Are Teams getting Lucky on Rushes?",
    "section": "Data",
    "text": "Data\nTo investigate these questions, I will be using the Passing Project Data from the 2015~2018 NHL seasons organized by Ryan Stimson and Corey Sznajder. The data was collected by a group of volunteers who tracked every pass leading to a shot. According to Ryan’s MIT Sloan Sports Analytics Conference paper, “Each tracker was required to complete 2 - 3 training games for validation by the project leader to ensure consistency in classification of passes and recording of data. Occasional spot-checking of games was also conducted.”\nThere are 44 variables and over 150,000 observations in this dataset. Some notable metrics that I will be using for my analysis include shot_on_goal (1 for yes and 0 for no), goal (1 for yes and 0 for no), odd_man (number of players involved in player advantages during rushes), shooting_percentage (number of goals / number of shots on goals), shot_type (one-timer, slap shot, wrist/snap shot, backhand, etc), a1_zone (zone that the primary assist originated), and rebound_shot_on_goal (if a rebound occured and resulted in a shot on goal).\nMany researchers have used this dataset to explore interesting research topics in hockey. For example, Ryan Stimson’s paper discusses repeatability of certain metrics on a team level and player level. Among other analysis, this NHL Numbers post covers repeatability of passes that crosses through the royal road (the line that goes directly through the middle of the ice from one net to the other) and provides visualizations.\nIn this analysis, I will look at repeatability of shots that were preceded by passes and taken during rushes with player advantages, shooting percentages, rebound shots, and rebound shooting percentages in 5v5 situations."
  },
  {
    "objectID": "posts/are-teams-getting-lucky-on-rushes/index.html#methods",
    "href": "posts/are-teams-getting-lucky-on-rushes/index.html#methods",
    "title": "Are Teams getting Lucky on Rushes?",
    "section": "Methods",
    "text": "Methods\nMy main analysis tools are scatterplots and linear regression lines. I use linear regression to examine the relationship between first-half statistics (x-axis on all plots below) and second-half statistics (y-axis on all plots below) within the same seasons at the team level. To do this, I split each of the 2015-16, 2016-17, and 2017-18 seasons into first-half (before Jan 1st) and second-half (after Jan 1st) and then calculated the quantities of interest at the team level within each half-season. I then matched up the team statistics from each half-season with its counterpart (e.g. Vancouver’s 2016-17 first-half statistics with Vancouver’s 2016-17 second-half statistics).\nI utilized Hadley Wickham’s ggplot2 package, a “system declaratively creating graphics, based on The Grammar of Graphics” to plot this information on a scatterplot and layered the regression line on each graph. I included 95% confidence bands, which provide a confidence interval on the slope of the regression line. In other words, we are 95% confident that the true regression line falls within the confidence bands, given the data we observed.\n\nIf I can draw a horizontal line completely within the confidence bands, this means there exists a regression line with a slope of 0 within the 95% confidence band. This indicates that there is no significant relationship between the x-variable and the y-variable.\nIf I cannot draw a horizontal line completely within the confidence bands, this means that a regression line with a slope of 0 falls outside the 95% confidence band. In other words, there is a significant relationship between the x-variable and the y-variable.\n\nIn each of the graphs below, there are three logos of each team, corresponding to each of the three seasons in the data. In terms of the linear regression graphs, the x-variable is a statistic from the first half of the season and the y-variable is a statistic from the second half of the season. As a result, we are really examining the relationship between the past and future of a statistic, or the repeatability of a statistic."
  },
  {
    "objectID": "posts/are-teams-getting-lucky-on-rushes/index.html#repeatability-of-odd-man-rush-shots",
    "href": "posts/are-teams-getting-lucky-on-rushes/index.html#repeatability-of-odd-man-rush-shots",
    "title": "Are Teams getting Lucky on Rushes?",
    "section": "Repeatability of Odd-Man Rush Shots",
    "text": "Repeatability of Odd-Man Rush Shots\n\n\n\n\n\n\n\n\n\n\n\nShots that were preceded by passes and taken on odd player rushes are not repeatable over a season; this signifies that the number of odd-player rush shots per game in the first half is not a good predictor of the number of odd-player rush shots per game in the second half.\nHowever, all other shots that were preceded by passes are repeatable, meaning that the number of shots per game in the first half is a good predictor of the number of shots per game in the second half.\n\n\nThese findings suggest that hockey management and coaches must not read too much into the (lack of) offensive opportunities during odd player rushes in the first half; the second half presents a whole new playing field for players when it comes to odd player rushes."
  },
  {
    "objectID": "posts/are-teams-getting-lucky-on-rushes/index.html#repeatability-of-shooting-percentage",
    "href": "posts/are-teams-getting-lucky-on-rushes/index.html#repeatability-of-shooting-percentage",
    "title": "Are Teams getting Lucky on Rushes?",
    "section": "Repeatability of Shooting Percentage",
    "text": "Repeatability of Shooting Percentage\nThe below analysis on shooting percentages is parallel to the argument presented by Adam Gretz in “The roller coaster world of NHL shooting percentages”. Gretz opens the article with this statement: “A player’s shooting percentage in a given season has more to do with his luck as a shooter than his skill or the way he’s actually playing.” Furthermore, he claims, “They’re a wild roller coaster ride from year to year (and even during the season) that are always going up and down.”\n\n\n\n\n\n\n\n\n\n\n\nThe scales for the two facetted graphs are vastly different: shooting percentages across the whole season are significantly higher in odd player rushes than all other shots. This discrepancy makes sense when we consider the offensive freedom players have during odd player rushes.\nUnlike shots per game played, shooting percentages for all other shots are not repeatable over a season.\nLike shots per game played, shooting percentages are not repeatable during odd player rushes, which means that teams that have had extremely low shooting perentages in the first half may not have a similar second half. On the other hand, teams with extremely high shooting percentages in the first half may not enjoy this in the second half!\n\n\nThese results confirm Gretz’s argument: “You just can’t count on a player to repeat or maintain a consistent shooting percentage, especially one that’s extremely high or, on the other end of the spectrum, extremely low.”"
  },
  {
    "objectID": "posts/are-teams-getting-lucky-on-rushes/index.html#repeatability-of-shooting-percentages-by-zone-of-primary-pass",
    "href": "posts/are-teams-getting-lucky-on-rushes/index.html#repeatability-of-shooting-percentages-by-zone-of-primary-pass",
    "title": "Are Teams getting Lucky on Rushes?",
    "section": "Repeatability of Shooting Percentages by Zone of Primary Pass",
    "text": "Repeatability of Shooting Percentages by Zone of Primary Pass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNone of these graphs show any signs of repeatability since I can draw a horizontal line through all these graphs.\nThus, the lane of a primary pass does not help explain the discrepancy in repeatability of shooting percentages.\n\n\nThis tells us that set plays are not going to yield consistent results. For example, a premeditated faceoff play, which involves a primary pass from the same lane everytime, is not going to yield the same proportion of goals from the first half to the second half."
  },
  {
    "objectID": "posts/are-teams-getting-lucky-on-rushes/index.html#repeatability-of-rebound-shots-per-gp",
    "href": "posts/are-teams-getting-lucky-on-rushes/index.html#repeatability-of-rebound-shots-per-gp",
    "title": "Are Teams getting Lucky on Rushes?",
    "section": "Repeatability of Rebound Shots per GP",
    "text": "Repeatability of Rebound Shots per GP\nHockey literature reveals that “goalies generally do not have an ability to control rebounds” (Pettapiece, 2018). This accounts for the unpredictability of rebounds over a season.\n\n\n\n\n\n\nThis graph shows that rebound shots are not repeatable over a given season; there is no significant association between first half and second half rebound shots per GP and any association is minor.\n\n\nThis suggests that rebounds are intrinsically hard to predict over an entire season. Teams face different styles of goaltenders and even different goaltenders if the team plays its backup or AHL goaltender. This graph seems to provide a new perspective, as it contradicts the findings of Schuckers’ 2016 paper: “Statistical Evaluation of Ice Hockey Goaltending”. Using 2009-2010, 2010-2011, 2011-2012, and 2012-2013 data, he suggests that “past rebound rates are not strongly predictive of future rates though they are consistent within a given season.” The above graph illustrates that rebound rates are not “consistent within a given season”."
  },
  {
    "objectID": "posts/are-teams-getting-lucky-on-rushes/index.html#repeatability-of-rebound-shooting-percentage",
    "href": "posts/are-teams-getting-lucky-on-rushes/index.html#repeatability-of-rebound-shooting-percentage",
    "title": "Are Teams getting Lucky on Rushes?",
    "section": "Repeatability of Rebound Shooting Percentage",
    "text": "Repeatability of Rebound Shooting Percentage\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nOverall, rebound shooting percentages are not repeatable since I can draw a horizontal line through the confidence band.\nAs can be seen in the graph below, the sample size (rebound shot on goal) is small and thus, I caution the reader from drawing a strong conclusion."
  },
  {
    "objectID": "posts/are-teams-getting-lucky-on-rushes/index.html#key-takeaways",
    "href": "posts/are-teams-getting-lucky-on-rushes/index.html#key-takeaways",
    "title": "Are Teams getting Lucky on Rushes?",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nA team’s rate of odd man rush shots that were preceded by passes is not repeatable within seasons\nA team’s rate of shots in other situations that were preceded by passes is repeatable within seasons.\nShooting percentages across the whole season are significantly higher in odd player rushes.\nShooting percentages during odd player rushes and other situations are not repeatable within seasons.\nThe lane of a primary pass does not lead to a discrepancy in repeatability of shooting percentages.\nRebound shots are not repeatable over a given season. Furthermore, rebound shooting percentages (rebound goals (goals scored from rebounds) divided by rebound shots) are not repeatable."
  },
  {
    "objectID": "posts/are-teams-getting-lucky-on-rushes/index.html#conclusion",
    "href": "posts/are-teams-getting-lucky-on-rushes/index.html#conclusion",
    "title": "Are Teams getting Lucky on Rushes?",
    "section": "Conclusion",
    "text": "Conclusion\nHockey is an unpredictable sport. This signifies that a team that may have a horrible record before January 1st still can bounce back to become one of the league’s best teams. Just ask the 2018 Arizona Coyotes. On Dec 31st, the Coyotes were 9-26-5 and were last in the league. “But since Feb. 8, only four NHL teams had more points than Arizona over than span.” The conclusion drawn from this analysis of different offensive statistics helps explain the resurgence of the Arizona Coyotes in the second half.\nI would like to thank Sam Ventura for his valuable advice.\nCode"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "NBA Chuckers\n\n\n\nShiny\n\n\n\nShiny App that visualizes Lineup Shot Distribution EFficiency from the 16/17 NBA Season\n\n\n\n\n\n\n\n\n\n\n\n\n\nNBA Expected Possession Value\n\n\n\nShiny\n\n\n\nShiny App that accompanies my blogpost called \"Animating Expected Possession Value in the NBA\"\n\n\n\n\n\n\n\n\n\n\n\n\n\nGgplot2: Elegant Graphics for Data Analysis Solutions\n\n\n\nBookdown\n\n\n\nSolutions Manual to the exercises in the 3rd edition of ggplot2: Elegant Graphics for Data Analysis written by [Hadley …\n\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Shiny Solutions\n\n\n\nBookdown\n\n\n\nSolutions Manual for the exercises in Mastering Shiny, written by Hadley Wickham\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample Shiny Dashboard\n\n\n\nShiny\n\n\n\nAn Example Shiny Dashboard I created called Maestro\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeedback Report\n\n\n\nShiny\n\n\n\nA Web-Based Shiny Dashboard for displaying Patient-Reported Outcome Measures for Patients in Addiction Treatment\n\n\n\n\n\n\n\n\n\n\n\n\n\nCure Model Simuation\n\n\n\nShiny\n\n\n\nSimulation of a Parametric Mixture Cure Model\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedium Clap Predictor App\n\n\n\nShiny\n\n\n\nShiny App that predicts the number of claps a blog post will receive solely based on the blog post title\n\n\n\n\n\n\n\n\n\n\n\n\n\nNHL Play-by-Play\n\n\n\nShiny\n\n\n\nShiny App that visualizes NHL Play-by-Play Data\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Howard Baik",
    "section": "",
    "text": "Howard Baik (he/him) is a Data Scientist I at the Yale School of Public Health."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Howard Baik",
    "section": "Education",
    "text": "Education\nOregon State University | Bachelor of Science in Computer Science | Sept, 2026\nUniversity of Washington | Seattle, WA Master of Science in Biostatistics | March, 2023\nUniversity of Washington | Seattle, WA Bachelor of Science in Statistics (Minor: Mathematics) | June, 2021"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Howard Baik",
    "section": "Experience",
    "text": "Experience\nYale School of Public Health | Data Scientist | September, 2024 - Present\nFred Hutch Data Science Lab | Software Development Engineer | March, 2023 - August, 2024\nFred Hutch Data Science Lab | Software Development Engineer | March, 2023 - August, 2024\nMerck | Contractor | March, 2022 - March, 2023\nMerck | Biostatistics Intern | June, 2022 - Aug, 2022"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a Software Development Engineer at the Fred Hutch Data Science Lab.\nI have a Master’s in Biostatistics from the University of Washington. My past experiences include an internship at Merck, a Machine Learning Internship at Northwest Fisheries Science Center (NWFSC), where I created an algorithm to detect an oceanographic process, NIH Research Assistantship at the Behavioral Research In Technology and Engineering (BRiTE) Center, where I developed a Shiny Dashboard that allows patients and clinicians in addiction treatment to monitor patients’ progress and goals over time, and an Educational Data Mining Research Internship at George Mason University, where I analyzed real world datasets of an online course at Stanford."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "My R Journal\n\n\n\nSoftware Engineering\n\n\n\nThings I learn along my R programming journey.\n\n\n\nHoward Baek\n\n\nJul 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Detection Method\n\n\n\nData Analysis\n\n\n\nWe introduce a highly effective algorithm for detecting upwelling, the automatic detection method using NOAA satellite image data.\n\n\n\nHoward Baek, Elizabeth Eli Holmes\n\n\nOct 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDemo of NHL Play-by-Play App\n\n\n\nData Visualization\n\n\n\nI will be showing insights from looking at a regular season game between the Montreal Canadiens and the Toronto Maple Leafs on October 3rd, 2018\n\n\n\nHoward Baek\n\n\nAug 20, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimating Expected Possession Value in the NBA Extended\n\n\n\nData Visualization\n\n\n\nI extend a MIT SSAC Research Paper by Cervone, DAmour, Bornn, and Goldsberry on Expected Possession Value or EPV.\n\n\n\nHoward Baek\n\n\nJun 15, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimating Expected Possession Value in the NBA\n\n\n\nData Visualization\n\n\n\nI extend a MIT SSAC Research Paper by Cervone, DAmour, Bornn, and Goldsberry on Expected Possession Value or EPV.\n\n\n\nHoward Baek\n\n\nApr 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the NHL PBP App in Shiny\n\n\n\nData Visualization\n\n\n\nI will walk through a month-long process building the National Hockey League Play-by-Play App from scratch, giving a behind-the-scenes look.\n\n\n\nHoward Baek\n\n\nFeb 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantifying Differences between the Regular Season and Playoffs Extended\n\n\n\nData Analysis\n\n\n\nA brief analysis to answer a question by Tom Tango on my article, Quantifying Differences between the Regular Season and the Playoffs\n\n\n\nHoward Baek\n\n\nJan 28, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantifying Differences between the Regular Season and Playoffs using Survival Analysis\n\n\n\nData Analysis\n\n\n\nFrom a casual fan perspective, the intensity traditionally ramps up in the playoffs because teams are closer to the grand prize, the Stanley Cup.\n\n\n\nHoward Baek\n\n\nJan 7, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Twitter Accounts\n\n\n\nData Analysis\n\n\n\nI grouped Twitter accounts by popularity measure, a quick and easy way to quantify the popularity of a Twitter account\n\n\n\nHoward Baek\n\n\nJan 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of an Educational Data Mining Paper\n\n\n\nResearch\n\n\n\nDiscovery and Temporal Analysis of Latent Study Patterns in MOOC Interaction Sequences\n\n\n\nHoward Baek\n\n\nJun 28, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAre Teams getting Lucky on Rushes?\n\n\n\nData Analysis\n\n\n\nWhen you watch an offensive rush in hockey, do you ever wonder about the numbers behind it?\n\n\n\nHoward Baek\n\n\nJun 28, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWine Quality Prediction\n\n\n\nResearch\n\n\n\nOn June 14th, 2018, I participated in a Data Hackathon as part of the REU program at George Mason University.\n\n\n\nHoward Baek\n\n\nJun 18, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  }
]